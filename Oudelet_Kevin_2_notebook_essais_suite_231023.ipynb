{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Segmentez des clients d'un site e-commerce**\n",
    "\n",
    "## partie 2/4, suite : essais de modélisation en 4 dimensions\n",
    "\n",
    "### <br> Résumé des épisodes précédents :\n",
    "\n",
    "> &emsp; \"Vous vous apprêtez à franchir un seuil, celui de la dimension inconnue, du mystère, <br>\n",
    "> de l'imagination. C'est le panneau indicateur à l'entrée de la Quatrième Dimension.\"<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Importation des librairies, réglages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import SpanSelector\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, inconsistent\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from IPython.display import Image\n",
    "# from zipfile import ZipFile\n",
    "\n",
    "print('Python version ' + sys.version)  # Print Python version\n",
    "print('\\npandas version ' + pd.__version__)\n",
    "print('sns version ' + sns.__version__)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', 200)\n",
    "sns.set(font_scale=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_look(df, miss=True):\n",
    "    \"\"\"\n",
    "    Display a quick overview of a DataFrame, including shape, head, tail, unique values, and duplicates.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to inspect.\n",
    "        check_missing (bool, optional): Whether to check and display missing values (default is True).\n",
    "\n",
    "    The function provides a summary of the DataFrame, including its shape, the first and last rows, the count of unique values per column, and the number of duplicates.\n",
    "    If `check_missing` is set to True, it also displays missing value information.\n",
    "    \"\"\"\n",
    "    print(f'shape : {df.shape}')\n",
    "\n",
    "    display(df.head())\n",
    "    display(df.tail())\n",
    "\n",
    "    print('uniques :')\n",
    "    display(df.nunique())\n",
    "\n",
    "    print('Doublons ? ', df.duplicated(keep='first').sum(), '\\n')\n",
    "\n",
    "    if miss:\n",
    "        display(get_missing_values(df))\n",
    "\n",
    "\n",
    "def lerp(a, b, t):\n",
    "    \"\"\"\n",
    "    Linear interpolation between two values 'a' and 'b' at a parameter 't'.\n",
    "    A very useful little function, used here to position annotations in plots.\n",
    "    Got it coding with Radu :)\n",
    "\n",
    "    Given two values 'a' and 'b', and a parameter 't',\n",
    "    this function calculates the linear interpolation between 'a' and 'b' at 't'.\n",
    "\n",
    "    Parameters:\n",
    "    a (float or int): The start value.\n",
    "    b (float or int): The end value.\n",
    "    t (float): The interpolation parameter (typically in the range [0, 1], but can be outside).\n",
    "\n",
    "    Returns:\n",
    "    float or int: The interpolated value at parameter 't'.\n",
    "    \"\"\"\n",
    "    return a + (b - a) * t\n",
    "\n",
    "\n",
    "def generate_random_pastel_colors(n):\n",
    "    \"\"\"\n",
    "    Generates a list of n random pastel colors, represented as RGBA tuples.\n",
    "\n",
    "    Parameters:\n",
    "    n (int): The number of pastel colors to generate.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of RGBA tuples representing random pastel colors.\n",
    "\n",
    "    Example:\n",
    "    >>> generate_random_pastel_colors(2)\n",
    "    [(0.749, 0.827, 0.886, 1.0), (0.886, 0.749, 0.827, 1.0)]\n",
    "    \"\"\"\n",
    "    colors = []\n",
    "    for _ in range(n):\n",
    "        # Generate random pastels\n",
    "        red = round(random.randint(150, 250) / 255.0, 3)\n",
    "        green = round(random.randint(150, 250) / 255.0, 3)\n",
    "        blue = round(random.randint(150, 250) / 255.0, 3)\n",
    "\n",
    "        # Create an RGB color tuple and add it to the list\n",
    "        color = (red,green,blue, 1.0)\n",
    "        colors.append(color)\n",
    "\n",
    "    return colors\n",
    "\n",
    "print(generate_random_pastel_colors(2))\n",
    "\n",
    "\n",
    "def get_missing_values(df):\n",
    "    \"\"\"Generates a DataFrame containing the count and proportion of missing values for each feature.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with columns for the feature name, count of missing values,\n",
    "        count of non-missing values, proportion of missing values, and data type for each feature.\n",
    "    \"\"\"\n",
    "    # Count the missing values for each column\n",
    "    missing = df.isna().sum()\n",
    "\n",
    "    # Calculate the percentage of missing values\n",
    "    percent_missing = df.isna().mean() * 100\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    missings_df = pd.DataFrame({\n",
    "        'column_name': df.columns,\n",
    "        'missing': missing,\n",
    "        'present': df.shape[0] - missing,  # Count of non-missing values\n",
    "        'percent_missing': percent_missing.round(2),  # Rounded to 2 decimal places\n",
    "        'type': df.dtypes\n",
    "    })\n",
    "\n",
    "    # Sort the DataFrame by the count of missing values\n",
    "    missings_df.sort_values('missing', inplace=True)\n",
    "\n",
    "    return missings_df\n",
    "\n",
    "# with pd.option_context('display.max_rows', 1000):\n",
    "#   display(get_missing_values(df))\n",
    "\n",
    "\n",
    "# ma fonction d'origine (non cleanée)\n",
    "def hist_distrib(dataframe, feature, bins, r, density=True):\n",
    "    \"\"\"\n",
    "    Affiche un histogramme, pour visualiser la distribution empirique d'une variable\n",
    "    Argument : df, feature num\n",
    "    \"\"\"\n",
    "    # calcul des tendances centrales :\n",
    "    mode =  str(round(dataframe[feature].mode()[0], r))\n",
    "    # mode is often zero, so Check if there are non nul values in the column\n",
    "    if (dataframe[feature] != 0).any():\n",
    "        mode_non_nul = str(round(dataframe.loc[dataframe[feature] != 0, feature].mode()[0], r))\n",
    "    else:\n",
    "        mode_non_nul = \"N/A\"\n",
    "    mediane = str(round(dataframe[feature].median(), r))\n",
    "    moyenne = str(round(dataframe[feature].mean(), r))\n",
    "    # dispersion :\n",
    "    var_emp = str(round(dataframe[feature].var(ddof=0), r))\n",
    "    coeff_var =  str(round(dataframe[feature].std(ddof=0), r)) # = écart-type empirique / moyenne\n",
    "    # forme\n",
    "    skewness = str(round(dataframe[feature].skew(), 2))\n",
    "    kurtosis = str(round(dataframe[feature].kurtosis(), 2))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    dataframe[feature].hist(density=density, bins=bins, ax=ax)\n",
    "    yt = plt.yticks()\n",
    "    y = lerp(yt[0][0], yt[0][-1], 0.8)\n",
    "    t = y/20\n",
    "    xt = plt.xticks()\n",
    "    x = lerp(xt[0][0], xt[0][-1], 0.7)\n",
    "    plt.title(feature, pad=20, fontsize=18)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    fs =13\n",
    "    plt.annotate('Mode : ' + mode, xy = (x, y), fontsize = fs, xytext = (x, y), color = 'g')\n",
    "    plt.annotate('Mode + : ' + mode_non_nul, xy = (x, y-t), fontsize = fs, xytext = (x, y-t), color = 'g')\n",
    "    plt.annotate('Médiane : ' + mediane, xy = (x, y-2*t), fontsize = fs, xytext = (x, y-2*t), color = 'g')\n",
    "    plt.annotate('Moyenne : ' + moyenne, xy = (x, y-3*t), fontsize = fs, xytext = (x, y-3*t), color = 'g')\n",
    "\n",
    "    plt.annotate('Var emp : ' + var_emp, xy = (x, y-5*t), fontsize = fs, xytext = (x, y-5*t), color = 'g')\n",
    "    plt.annotate('Coeff var : ' + coeff_var, xy = (x, y-6*t), fontsize = fs, xytext = (x, y-6*t), color = 'g')\n",
    "\n",
    "    plt.annotate('Skewness : ' + skewness, xy = (x, y-8*t), fontsize = fs, xytext = (x, y-8*t), color = 'g')\n",
    "    plt.annotate('Kurtosis : ' + kurtosis, xy = (x, y-9*t), fontsize = fs, xytext = (x, y-9*t), color = 'g')\n",
    "    plt.show()\n",
    "\n",
    "    return float(skewness) # pour eventuel passage au log\n",
    "\n",
    "# version cleanée\n",
    "def hist_distrib(dataframe, feature, bins, decimal_places, density=True):\n",
    "    \"\"\"\n",
    "    Visualize the empirical distribution of a numerical feature using a histogram.\n",
    "    Calcul des principaux indicateurs de tendance centrale, dispersion et forme.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): The input DataFrame containing the feature.\n",
    "        feature (str): The name of the numerical feature to visualize.\n",
    "        bins (int): The number of bins for the histogram.\n",
    "        decimal_places (int): The number of decimal places for rounding numeric values.\n",
    "        density (bool, optional): Whether to display the histogram as a density plot (default is True).\n",
    "\n",
    "    Returns:\n",
    "        float: The skewness of the feature's distribution.\n",
    "\n",
    "    The function generates a histogram of the feature, displays various statistics, and returns the skewness of the distribution.\n",
    "    \"\"\"\n",
    "    # Calculate central tendencies and dispersion\n",
    "    mode_value = round(dataframe[feature].mode()[0], decimal_places)\n",
    "    mode_non_zero = \"N/A\"\n",
    "    if (dataframe[feature] != 0).any():\n",
    "        mode_non_zero = round(dataframe.loc[dataframe[feature] != 0, feature].mode()[0], decimal_places)\n",
    "    median_value = round(dataframe[feature].median(), decimal_places)\n",
    "    mean_value = round(dataframe[feature].mean(), decimal_places)\n",
    "\n",
    "    # Calculate dispersion\n",
    "    var_emp = round(dataframe[feature].var(ddof=0), decimal_places)\n",
    "    coeff_var = round(dataframe[feature].std(ddof=0), decimal_places)\n",
    "\n",
    "    # Calculate shape indicators\n",
    "    skewness_value = round(dataframe[feature].skew(), 2)\n",
    "    kurtosis_value = round(dataframe[feature].kurtosis(), 2)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    dataframe[feature].hist(density=density, bins=bins, ax=ax)\n",
    "\n",
    "    # Adjust placement for annotations\n",
    "    yt = plt.yticks()\n",
    "    y_position = lerp(yt[0][0], yt[0][-1], 0.8)\n",
    "    y_increment = y_position / 20\n",
    "    xt = plt.xticks()\n",
    "    x_position = lerp(xt[0][0], xt[0][-1], 0.7)\n",
    "\n",
    "    # Add annotations with horizontal and vertical alignment\n",
    "    annotation_fs = 13\n",
    "    color = 'g'\n",
    "    ax.annotate(f'Mode: {mode_value}', xy=(x_position, y_position), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position), color=color, ha='left', va='bottom')\n",
    "    ax.annotate(f'Mode +: {mode_non_zero}', xy=(x_position, y_position - y_increment), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position - y_increment), color=color, ha='left', va='bottom')\n",
    "    ax.annotate(f'Median: {median_value}', xy=(x_position, y_position - 2 * y_increment), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position - 2 * y_increment), color=color, ha='left', va='bottom')\n",
    "    ax.annotate(f'Mean: {mean_value}', xy=(x_position, y_position - 3 * y_increment), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position - 3 * y_increment), color=color, ha='left', va='bottom')\n",
    "    ax.annotate(f'Var Emp: {var_emp}', xy=(x_position, y_position - 5 * y_increment), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position - 5 * y_increment), color=color, ha='left', va='bottom')\n",
    "    ax.annotate(f'Coeff Var: {coeff_var}', xy=(x_position, y_position - 6 * y_increment), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position - 6 * y_increment), color=color, ha='left', va='bottom')\n",
    "    ax.annotate(f'Skewness: {skewness_value}', xy=(x_position, y_position - 8 * y_increment), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position - 8 * y_increment), color=color, ha='left', va='bottom')\n",
    "    ax.annotate(f'Kurtosis: {kurtosis_value}', xy=(x_position, y_position - 9 * y_increment), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position - 9 * y_increment), color=color, ha='left', va='bottom')\n",
    "\n",
    "    # Label the x-axis and y-axis\n",
    "    ax.set_xlabel(feature, fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.title(f'Distribution of {feature}', pad=20, fontsize=18)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    return skewness_value\n",
    "\n",
    "\n",
    "def boxplot_distrib(dataframe, feature):\n",
    "    \"\"\"\n",
    "    Affiche un boxplot, pour visualiser les tendances centrales et la dispersion d'une variable.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): The input DataFrame containing the feature.\n",
    "        feature (str): The name of the numerical feature to visualize.\n",
    "\n",
    "    The function generates a box plot of the feature to display central tendencies (median and mean) and dispersion.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "    medianprops = {'color':\"blue\"}\n",
    "    meanprops = {'marker':'o', 'markeredgecolor':'black',\n",
    "            'markerfacecolor':'firebrick'}\n",
    "\n",
    "    dataframe.boxplot(feature, vert=False, showfliers=False, medianprops=medianprops, patch_artist=True, showmeans=True, meanprops=meanprops)\n",
    "\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def courbe_lorenz(dataframe, feature):\n",
    "    \"\"\"\n",
    "    Affiche une courbe de Lorenz, pour visualiser la concentration d'une variable\n",
    "    Calcule l'indice de Gini\n",
    "    Visualize a Lorenz curve to assess the concentration of a variable and calculate the Gini coefficient.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): The input DataFrame containing the feature.\n",
    "        feature (str): The name of the numerical feature to visualize.\n",
    "\n",
    "    The function generates a Lorenz curve to assess the concentration of the feature and calculates the Gini coefficient.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    values = dataframe.loc[dataframe[feature].notna(), feature].values\n",
    "    # print(values)\n",
    "    n = len(values)\n",
    "    lorenz = np.cumsum(np.sort(values)) / values.sum()\n",
    "    lorenz = np.append([0],lorenz) # La courbe de Lorenz commence à 0\n",
    "\n",
    "    xaxis = np.linspace(0-1/n,1+1/n,n+1)\n",
    "    #Il y a un segment de taille n pour chaque individu, plus 1 segment supplémentaire d'ordonnée 0.\n",
    "    # #Le premier segment commence à 0-1/n, et le dernier termine à 1+1/n.\n",
    "    plt.plot(xaxis,lorenz,drawstyle='steps-post')\n",
    "    plt.plot(np.arange(2),[x for x in np.arange(2)])\n",
    "    # calcul de l'indice de Gini\n",
    "    AUC = (lorenz.sum() -lorenz[-1]/2 -lorenz[0]/2)/n # Surface sous la courbe de Lorenz. Le premier segment (lorenz[0]) est à moitié en dessous de 0, on le coupe donc en 2, on fait de même pour le dernier segment lorenz[-1] qui est à moitié au dessus de 1.\n",
    "    S = 0.5 - AUC # surface entre la première bissectrice et le courbe de Lorenz\n",
    "    gini = 2*S\n",
    "    plt.annotate('gini =  ' + str(round(gini, 2)), xy = (0.04, 0.88), fontsize = 13, xytext = (0.04, 0.88), color = 'g')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def graphs_analyse_uni(dataframe, feature, bins=50, r=5, density=True):\n",
    "    \"\"\"\n",
    "    Affiche histogramme + boxplot + courbe de Lorenz\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): The input DataFrame containing the feature.\n",
    "        feature (str): The name of the numerical feature to analyze.\n",
    "        bins (int, optional): The number of bins for the histogram (default is 50).\n",
    "        decimal_places (int, optional): The number of decimal places for rounding numeric values (default is 5).\n",
    "        density (bool, optional): Whether to display the histogram as a density plot (default is True).\n",
    "\n",
    "    The function generates and displays an analysis of the given numerical feature, including an histogram, a box plot, and a Lorenz curve.\n",
    "    \"\"\"\n",
    "    hist_distrib(dataframe, feature, bins, r)\n",
    "    boxplot_distrib(dataframe, feature)\n",
    "    courbe_lorenz(dataframe, feature)\n",
    "\n",
    "\n",
    "def shape_head(df, nb_rows=5):\n",
    "    \"\"\"\n",
    "    Affiche les dimensions et les premières lignes dùun dataframe\n",
    "    Display the dimensions and the first rows of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to display.\n",
    "        nb_rows (int, optional): The number of rows to display (default is 5, max is 60).\n",
    "\n",
    "    The function prints the dimensions of the DataFrame and displays the first few rows.\n",
    "    \"\"\"\n",
    "    print(df.shape)\n",
    "    display(df.head(nb_rows))\n",
    "\n",
    "\n",
    "def doughnut(df, feature, title, width=10, height=10):\n",
    "    \"\"\"\n",
    "    Affiche la répartition d'une feature sous forme de diagramme circulaire\n",
    "    Display the distribution of a feature as a doughnut chart.\n",
    "    Les couleurs sont aléatoires.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame containing the feature.\n",
    "        feature (str): The name of the feature to visualize.\n",
    "        title (str): The title for the doughnut chart.\n",
    "        width (int, optional): The width of the chart (default is 10).\n",
    "        height (int, optional): The height of the chart (default is 10).\n",
    "\n",
    "    The function creates a doughnut chart to visualize the distribution of the specified feature.\n",
    "    If you don't like the colors, try running it again :)\n",
    "    \"\"\"\n",
    "    colors = generate_random_pastel_colors(20)\n",
    "\n",
    "    grouped_df = df.groupby(feature).size().to_frame(\"count_per_type\").reset_index()\n",
    "    pie = grouped_df.set_index(feature).copy()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width, height))\n",
    "\n",
    "    patches, texts, autotexts = plt.pie(x=pie['count_per_type'], autopct='%1.1f%%',\n",
    "        startangle=-30, labels=pie.index, textprops={'fontsize':11, 'color':'#000'},\n",
    "        labeldistance=1.25, pctdistance=0.85, colors=colors)\n",
    "\n",
    "    plt.title(\n",
    "    label=title,\n",
    "    fontdict={\"fontsize\":17},\n",
    "    pad=20\n",
    "    )\n",
    "\n",
    "    for text in texts:\n",
    "        # text.set_fontweight('bold')\n",
    "        text.set_horizontalalignment('center')\n",
    "\n",
    "    # Customize percent labels\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_horizontalalignment('center')\n",
    "        autotext.set_fontstyle('italic')\n",
    "        autotext.set_fontsize('10')\n",
    "\n",
    "    #draw circle\n",
    "    centre_circle = plt.Circle((0,0),0.7,fc='white')\n",
    "    fig = plt.gcf()\n",
    "    fig.gca().add_artist(centre_circle)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_non_null_values(df):\n",
    "    \"\"\"\n",
    "    Génère un dataframe contenant le nombre et la proportion de non-null (non-zero) valeurs pour chaque feature\n",
    "    Generate a DataFrame containing the count and proportion of non-null (non-zero) values for each feature.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to analyze.\n",
    "\n",
    "    The function calculates and returns a DataFrame with the count and percentage of non-null (non-zero) values for each feature.\n",
    "    \"\"\"\n",
    "    non_null_counts = df.ne(0).sum()\n",
    "    percent_non_null = (non_null_counts / df.shape[0]) * 100\n",
    "    non_null_values_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                       'non_null_count': non_null_counts,\n",
    "                                       'percent_non_null': percent_non_null.round(2),\n",
    "                                       'type': df.dtypes})\n",
    "    non_null_values_df.sort_values('non_null_count', inplace=True)\n",
    "    return non_null_values_df\n",
    "\n",
    "\n",
    "def get_colors(n=7):\n",
    "    \"\"\"\n",
    "    Generate a list of random colors from multiple colormaps.\n",
    "\n",
    "    Args:\n",
    "        n (int, optional): The number of colors to sample from each colormap (default is 7).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of random colors sampled from different colormaps.\n",
    "    \"\"\"\n",
    "    num_colors_per_colormap = n\n",
    "    colormaps = [plt.cm.Pastel2, plt.cm.Set1, plt.cm.Paired]\n",
    "    all_colors = []\n",
    "\n",
    "    for colormap in colormaps:\n",
    "        colors = colormap(np.linspace(0, 1, num_colors_per_colormap))\n",
    "        all_colors.extend(colors)\n",
    "\n",
    "    np.random.shuffle(all_colors)\n",
    "\n",
    "    return all_colors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfms_complet = pd.read_csv('data/rfms.csv', sep=',')\n",
    "\n",
    "rfms_complet['datetime'] = pd.to_datetime(rfms_complet['datetime'])\n",
    "\n",
    "quick_look(rfms_complet, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfms = rfms_complet.loc[rfms_complet['satisfaction'].notna(), :].copy()\n",
    "\n",
    "quick_look(rfms, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toute la data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D scatter plot\n",
    "fig = px.scatter_3d(rfms, x='recent_timestamp', y='nb_commandes', z='payment_total')\n",
    "\n",
    "# Customize the plot\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(title='Our data in 3D')\n",
    "\n",
    "fig.update_layout(\n",
    "    width=800,  # Specify the width in pixels\n",
    "    height=600  # Specify the height in pixels\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouvelle feature : review score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfms['round_sat'] = np.round(rfms['satisfaction']) # for simple visualisation\n",
    "hist_distrib(rfms, 'round_sat', 5, 2)\n",
    "\n",
    "# Colorons maintenant en fonction du review_score\n",
    "custom_color_set_5 = ['red', 'purple', 'blue', 'lightgreen', 'yellow']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "N, bins, patches = ax.hist(rfms['round_sat'], density=False, bins=5)\n",
    "plt.title('review_scores moyens', pad=20, fontsize=18)\n",
    "\n",
    "for i, color in enumerate(custom_color_set_5):\n",
    "    patches[i].set_facecolor(color)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problème couleurs\n",
    "\n",
    "def plot_data_3D_hue(df=rfms, legend=True, color_set=custom_color_set_5, opacity=1.0):\n",
    "    # np.sort(df['round_sat'].unique())\n",
    "    labels_str = df['round_sat'].astype(str)\n",
    "    colors = color_set[:len(set(df['round_sat']))]\n",
    "    # Create a 3D scatter plot with custom legend\n",
    "    fig = px.scatter_3d(df, x='recent_timestamp', y='nb_commandes', z='payment_total', color=labels_str,\n",
    "                        color_discrete_sequence=colors, opacity=opacity)\n",
    "\n",
    "    fig.update_traces(marker=dict(size=5), showlegend=legend)\n",
    "    fig.update_layout(title='Our data')\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=800,  # Specify the width in pixels\n",
    "        height=600  # Specify the height in pixels\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_data_3D_hue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK\n",
    "\n",
    "def plot_data_3D_hue(df=rfms):\n",
    "    subset = []\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        subset.append(df.loc[df['round_sat'] == i, :].copy())\n",
    "        print(f'Subset review_score moyen == {i} : {subset[i-1].shape[0]} clients')\n",
    "\n",
    "        scatter = go.Scatter3d(x=subset[i-1]['recent_timestamp'],\n",
    "                            y=subset[i-1]['nb_commandes'],\n",
    "                            z=subset[i-1]['payment_total'],\n",
    "                            mode='markers',\n",
    "                            marker=dict(size=5, color=custom_color_set_5[i-1]),\n",
    "                            name=f'score={i}')  # Set the legend label here\n",
    "        fig.add_trace(scatter)\n",
    "\n",
    "    fig.update_layout(title=f'Our data in 3D+ Subset {i}')\n",
    "    fig.update_layout(width=600, height=400)\n",
    "    fig.update_scenes(xaxis_title_text='recent_timestamp', # Set the labels\n",
    "                yaxis_title_text='nb_commandes',\n",
    "                zaxis_title_text='payment_total')\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "plot_data_3D_hue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_4D(df=rfms):\n",
    "    subset = []\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        subset.append(df.loc[df['round_sat'] == i, :].copy())\n",
    "        print(f'Subset review_score moyen == {i} : {subset[i-1].shape[0]} clients')\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        scatter = go.Scatter3d(x=subset[i-1]['recent_timestamp'],\n",
    "                            y=subset[i-1]['nb_commandes'],\n",
    "                            z=subset[i-1]['payment_total'],\n",
    "                            mode='markers',\n",
    "                            marker=dict(size=5, color=custom_color_set_5[i-1]),\n",
    "                            name=f'score={i}')  # Set the legend label here\n",
    "\n",
    "        fig.add_trace(scatter)\n",
    "        fig.update_layout(title=f'Our data in 3D+ Subset {i}')\n",
    "        fig.update_layout(width=500, height=400)\n",
    "        fig.update_scenes(xaxis_title_text='recent_timestamp', # Set the labels\n",
    "                    yaxis_title_text='nb_commandes',\n",
    "                    zaxis_title_text='payment_total')\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "plot_data_4D(df=rfms)\n",
    "\n",
    "# Pas forcément évident parce qu'on a bcp de points, mais on voit ici comment la data est étirée\n",
    "# le long d'un nouvel axe, à chaque fois qu'on prend en compte une nouvelle feature dans notre analyse\n",
    "\n",
    "# En 5 dimensions, on peut encore visualiser, en affichant un plan de projections 3D\n",
    "# Au-delà ça va devenir + technique avec les limitations d'un notebook.\n",
    "# Note de futur : c mm pire que ce que j'imaginais\n",
    "\n",
    "# Ici on dirait que les clients les + satisfaits (jaune) ont fait relativement peu de commandes,\n",
    "# par rapport aux aux clients moins satisfaits.\n",
    "\n",
    "# C'est à la fois frappant, car ce groupe représente + de la moitié du dataset total,\n",
    "# et inquiétant pour Olis : il semble que + les clients commandent, + le review_score baisse\n",
    "# (à vérifier. Si confirmation, cela demande à être étudié)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution du review score en fonction de nb de commandes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfms['count_clients'] = 1\n",
    "\n",
    "df_satisfaction = rfms.groupby('nb_commandes').agg({\n",
    "    'satisfaction': 'mean',\n",
    "    'count_clients': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# feature passée au log\n",
    "df_satisfaction['vrai_nb_commandes'] = np.exp(df_satisfaction['nb_commandes'])\n",
    "\n",
    "max_nb_commandes = df_satisfaction['vrai_nb_commandes'].max()\n",
    "print(max_nb_commandes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))  # Adjust the figsize as needed\n",
    "\n",
    "df_satisfaction.plot(kind='line', x='vrai_nb_commandes', y='satisfaction', fontsize=12, color='b', ax=ax)\n",
    "# df_satisfaction.plot(kind='line', x='vrai_nb_commandes', y='count_clients', fontsize=12, color='r', ax=ax)\n",
    "plt.title('Satisfaction / nb_commandes', fontsize=18, pad=20)\n",
    "plt.legend(loc='upper left', fontsize=12)\n",
    "plt.xticks(fontsize=7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risque d'attrition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_satisfaction['count_clients'] = df_satisfaction['count_clients'] / 10000       # (visu)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))  # Adjust the figsize as needed\n",
    "\n",
    "df_satisfaction.plot(kind='line', x='vrai_nb_commandes', y='satisfaction', fontsize=12, color='b', ax=ax)\n",
    "df_satisfaction.plot(kind='line', x='vrai_nb_commandes', y='count_clients', fontsize=12, color='r', ax=ax)\n",
    "plt.title('Satisfaction / nb_commandes', fontsize=18, pad=20)\n",
    "plt.legend(loc='upper left', fontsize=12)\n",
    "plt.xticks(fontsize=7)\n",
    "plt.show()\n",
    "\n",
    "# Ce n'était pas qu'une impression, la satisfaction myenne chute après 8-9 commandes,\n",
    "# puis remonte régulièrement. Sans s'avancer sur des hypothèses qd aux causes,\n",
    "# (par exemple, il pourrait simplement s'agir de la proba qu'une livraison finisse par mal se passer)\n",
    "# je trouve que c'est un point intéressant à mentionner à Olis :\n",
    "# on a une zone rouge après 7 commandes où le risque d'attrition (churning) est élevé,\n",
    "# il faut donc mettre en place des mesures préventives.\n",
    "# Ceci dit cela ne concerne que très peu de clients, une petite fraction des \"3%\" de clients \"réguliers\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forme des samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notre subset au 1/10eme capture-t-il tjs la forme générale... en 4 dimensions ?\n",
    "\n",
    "plot_data_4D(df=rfms[::10])\n",
    "\n",
    "# Ca a l'air pas trop mal, sauf pour les rouges qui perdent les valeurs moyennes sur l'axe nb_commandes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essayons d'ajouter de l'aléatoire dans notre méthode de sélection\n",
    "\n",
    "features_RFMS = ['recent_timestamp', 'nb_commandes', 'payment_total', 'satisfaction']\n",
    "\n",
    "# Shuffle the DataFrame randomly\n",
    "shuffled_df = rfms[features_RFMS + ['round_sat']].sample(frac=1, random_state=i)  # frac=1 shuffles all rows\n",
    "\n",
    "# Sample a portion of the shuffled DataFrame\n",
    "sample_size = 10000\n",
    "sampled_df = shuffled_df.head(sample_size)\n",
    "\n",
    "plot_data_4D(df=sampled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On a tjs des trous, en 4D le nombre d'individus n'est plus suffisant pour couvrir l'espace.\n",
    "\n",
    "sample_size = 20000\n",
    "sampled_df = shuffled_df.head(sample_size)\n",
    "\n",
    "plot_data_4D(df=sampled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Même au 1/5ème, la topologie est déformée. Des vides apparaissent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "rfms_scaled = rfms.copy()\n",
    "\n",
    "rfms_scaled[features_RFMS + ['round_sat']] = scaler.fit_transform(rfms[features_RFMS + ['round_sat']])\n",
    "\n",
    "hist_distrib(rfms_scaled, 'satisfaction', 5, 2)\n",
    "hist_distrib(rfms_scaled, 'round_sat', 5, 2)\n",
    "# OK, same distribs\n",
    "\n",
    "# C quoi la fonction de normalisation qd une feature a un skew négatif (asymétrie left tail) ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 kmeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nombre optimal de clusters using different scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the score values for different numbers of clusters\n",
    "def plot_score(score_list, score_name, max=15):\n",
    "    \"\"\"\n",
    "    Plot the score values for different numbers of clusters.\n",
    "\n",
    "    Args:\n",
    "        score_list (list): A list of score values for different numbers of clusters.\n",
    "        score_name (str): The name of the score being plotted (e.g., 'Silhouette Score').\n",
    "        max (int, optional): The maximum number of clusters to plot (default is 15).\n",
    "\n",
    "    This function generates a line plot to visualize how a specific clustering score varies with different numbers of clusters (k).\n",
    "    It helps in identifying the optimal number of clusters using the given clustering score.\n",
    "\n",
    "    Parameters:\n",
    "    - score_list: A list of score values for different values of k.\n",
    "    - score_name: The name of the score to display in the plot (e.g., 'Silhouette Score', 'Inertia', etc.).\n",
    "    - max: The maximum number of clusters to plot (default is 15).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(2, max), score_list, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel(score_name)\n",
    "    plt.title('Optimal k, using ' + score_name + ' score')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Calculate Tightness\n",
    "def compute_tightness(data, cluster_labels, cluster_centers):\n",
    "    \"\"\"\n",
    "    Calculate the tightness score for a clustering.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): The data points.\n",
    "        cluster_labels (numpy.ndarray): Labels indicating the cluster assignment for each data point.\n",
    "        cluster_centers (numpy.ndarray): The centers of the clusters.\n",
    "\n",
    "    Returns:\n",
    "        float: The tightness score, a measure of how close data points are to their cluster centers.\n",
    "\n",
    "    The tightness score quantifies the compactness of clusters in a clustering. It is computed as the average distance of data points within each cluster to their respective cluster center. A lower tightness score indicates that data points are closer to their cluster centers, indicating tighter clusters.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The data points in the dataset.\n",
    "    - cluster_labels: An array of cluster labels for each data point.\n",
    "    - cluster_centers: The centers of the clusters.\n",
    "    \"\"\"\n",
    "    tightness_score = 0\n",
    "    for i in range(len(cluster_centers)):\n",
    "        cluster_points = data[cluster_labels == i]\n",
    "        center = cluster_centers[i]\n",
    "        distances = np.linalg.norm(cluster_points - center, axis=1)\n",
    "        tightness_score += np.mean(distances)\n",
    "    tightness_score /= len(cluster_centers)\n",
    "    return tightness_score\n",
    "\n",
    "def find_best_nb_clusters_for_kmeans(df=rfms_scaled, features=features_RFMS, scores=[], max=15):\n",
    "    \"\"\"\n",
    "    Find the best number of clusters for K-Means clustering based on various scoring methods.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset for clustering.\n",
    "        features (list): List of feature columns for clustering.\n",
    "        scores (list): List of scoring methods to use ('inertia', 'tightness', 'db', 'silhouette').\n",
    "        max (int): The maximum number of clusters to consider (default is 15).\n",
    "\n",
    "    This function performs K-Means clustering with different numbers of clusters (k) and evaluates the clustering results using various scoring methods, including inertia, tightness, Davies-Bouldin index, and silhouette score. It then plots the scores to help determine the optimal number of clusters based on the chosen scoring methods.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The dataset for clustering.\n",
    "    - features: List of feature columns for clustering.\n",
    "    - scores: List of scoring methods to use ('inertia', 'tightness', 'db', 'silhouette').\n",
    "    - max: The maximum number of clusters to consider (default is 15).\n",
    "    \"\"\"\n",
    "    # Create empty lists to store scores for different k values\n",
    "    inertia_scores = []\n",
    "    tightness_scores = []\n",
    "    davies_bouldin_scores = []\n",
    "    silhouette_scores = []\n",
    "\n",
    "    # Test different numbers of clusters from 2 to 15\n",
    "    for k in range(2, max):\n",
    "        kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "        kmeans.fit(df[features])\n",
    "\n",
    "        if 'inertia' in scores:\n",
    "            inertia_scores.append(kmeans.inertia_)\n",
    "        if 'tightness' in scores:\n",
    "            tightness = compute_tightness(df[features], kmeans.labels_, kmeans.cluster_centers_)\n",
    "            tightness_scores.append(tightness)\n",
    "        if 'db' in scores:\n",
    "            davies_bouldin = davies_bouldin_score(df[features], kmeans.labels_)\n",
    "            davies_bouldin_scores.append(davies_bouldin)\n",
    "        if 'silhouette' in scores:\n",
    "            silhouette = silhouette_score(df[features], kmeans.labels_)\n",
    "            silhouette_scores.append(silhouette)\n",
    "\n",
    "    if 'inertia' in scores:\n",
    "        plot_score(inertia_scores, 'inertia', max=max)\n",
    "    if 'tightness' in scores:\n",
    "        plot_score(tightness_scores, 'tightness', max=max)\n",
    "    if 'db' in scores:\n",
    "        plot_score(davies_bouldin_scores, 'Davies-Bouldin', max=max)\n",
    "    if 'silhouette' in scores:\n",
    "        plot_score(silhouette_scores, 'silhouette', max=max)\n",
    "\n",
    "\n",
    "find_best_nb_clusters_for_kmeans(scores=['inertia'], max=15)\n",
    "\n",
    "# best nb de cluster : 5 (elbow method)\n",
    "# tps : 6s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_nb_clusters_for_kmeans(scores=['tightness'], max=15)\n",
    "\n",
    "# best nb de cluster : 5-6 (coude), 11 (min)\n",
    "# tps : 6-7 s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_nb_clusters_for_kmeans(df=rfms_scaled, scores=['db'], max=15)\n",
    "\n",
    "# best nb de cluster : 5 (min)\n",
    "# tps : 6s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sur dataset complet, compter 68 minutes (thermostat 6)\n",
    "# find_best_nb_clusters_for_kmeans(df=rfms_scaled, scores=['silhouette'], max=15)\n",
    "\n",
    "# Sur le subset 1/10, 20-25 secondes\n",
    "# La courbe est assez différente\n",
    "# find_best_nb_clusters_for_kmeans(df=rfms_scaled[::10], scores=['silhouette'], max=15)\n",
    "\n",
    "# best nb de cluster : 5 (max, elbow)\n",
    "display(Image(filename='img/silhouette_4D.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yellowbrick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans(n_init='auto')\n",
    "visualizer = KElbowVisualizer(model, k=(2,12))\n",
    "\n",
    "visualizer.fit(rfms_scaled[features_RFMS])    # Fit the data to the visualizer\n",
    "visualizer.poof()    # Draw/show/poof the data\n",
    "\n",
    "# bcp, bcp + rapide que notre méthode (silhouette_score, from sklearn.metrics)\n",
    "# Comment ils font ?\n",
    "\n",
    "# best nb clusters = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans(n_init='auto')\n",
    "visualizer = KElbowVisualizer(model, k=(2,12), metric='calinski_harabasz', timings=False)\n",
    "\n",
    "visualizer.fit(rfms_scaled[features_RFMS])    # Fit the data to the visualizer\n",
    "visualizer.poof()    # Draw/show/poof the data\n",
    "\n",
    "# 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beaucoup de scores concordants pour best number of clusters = 5 ici\n",
    "\n",
    "model = KMeans(5, n_init='auto')\n",
    "visualizer = SilhouetteVisualizer(model)\n",
    "\n",
    "# environ 6 min sur dataset complet\n",
    "# visualizer.fit(rfms_scaled[features_RFMS])    # Fit the data to the visualizer\n",
    "# visualizer.poof()    # Draw/show/poof the data\n",
    "\n",
    "# tjs pas terrible (en termes de séparation / régularité des clusters)\n",
    "# (1 cluster a bcp moins d'étendue, d'inertie) (= nos 3% ?)\n",
    "# comparer au resultat avec 4 clusters (oubli, en-dessous) : bcp + équilibré using best k :)\n",
    "\n",
    "display(Image(filename='img/poof_rfms5.png'))\n",
    "display(Image(filename='img/poof_rfms4.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stabilité\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etude de la stabilité des clusters sur une sous-partition aléatoire\n",
    "# le nombre de clusters proposés reste (étonnament ?) stable\n",
    "\n",
    "def not_sure_why_im_doin_this_one():\n",
    "    n_iter = 10\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # Shuffle the DataFrame randomly\n",
    "        shuffled_df = rfms_scaled[features_RFMS].sample(frac=1, random_state=i)  # frac=1 shuffles all rows\n",
    "\n",
    "        # Sample a portion of the shuffled DataFrame\n",
    "        sample_size = 5000  # Adjust this to your desired sample size\n",
    "        sampled_df = shuffled_df.head(sample_size)\n",
    "\n",
    "        inertia = []\n",
    "\n",
    "        for k in range(2, 11):\n",
    "            kmeans = KMeans(n_clusters=k, n_init=10, random_state=i)\n",
    "            kmeans.fit(shuffled_df[features_RFMS])\n",
    "\n",
    "            inertia.append(kmeans.inertia_)\n",
    "\n",
    "        print(inertia)\n",
    "\n",
    "        # Plot the inertia values for different numbers of clusters\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(range(2, 11), inertia, marker='o', linestyle='-', color='b')\n",
    "        plt.xlabel('Number of Clusters (k)')\n",
    "        plt.ylabel('Inertia')\n",
    "        plt.title('Elbow Method for Optimal k')\n",
    "        plt.grid(True)\n",
    "\n",
    "# not_sure_why_im_doin_this_one()\n",
    "\n",
    "# Tjs très stable !\n",
    "# Vérifier mieux que les clusters sont proches, similaires : -> utiliser l'ARI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous allons avoir besoin d'une fonction qui retourne les labels assignés par le k-means\n",
    "# (pour visualisation et ARI).\n",
    "\n",
    "# Assuming 5 is the optimal number of clusters\n",
    "def assign_labels_using_kmeans(alea, df=rfms_scaled, features=features_RFMS, best_k=5, silhouette=False):\n",
    "    \"\"\"\n",
    "    Assign cluster labels to data points using K-Means clustering.\n",
    "\n",
    "    Args:\n",
    "        alea (int): Random seed for reproducibility.\n",
    "        df (pd.DataFrame): The dataset for clustering.\n",
    "        features (list): List of feature columns for clustering.\n",
    "        best_k (int): The number of clusters for K-Means (default is 5).\n",
    "        silhouette (bool): Whether to print the silhouette score for the K-Means clustering (default is False).\n",
    "\n",
    "    This function performs K-Means clustering on the given data with a specified number of clusters (best_k) and assigns cluster labels to data points. Optionally, it can print the silhouette score as a measure of clustering quality.\n",
    "\n",
    "    Returns:\n",
    "    - labels (array): Cluster labels assigned to data points.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=best_k, n_init='auto', random_state=alea)\n",
    "    kmeans.fit(df[features])\n",
    "\n",
    "    if silhouette == True:\n",
    "        print('silhouette_score for kmeans = ', silhouette_score(df[features], kmeans.labels_))\n",
    "\n",
    "    return kmeans.labels_\n",
    "\n",
    "# L'ARI est souvent utilisé pour comparer le clustering obtenu par un modèle à une partition connue,\n",
    "# pour tester la qualité du modèle. Ici c'est un peu différent,\n",
    "# Nous n'avons pas de partition pré-établie. Nous allons seulement tester la stabilité,\n",
    "# pas la qualité, en comparant les labels obtenus successivement à ceux de l'essai précédent.\n",
    "\n",
    "labels_kmeans = []\n",
    "\n",
    "for i in range(0, 15):\n",
    "    labels = assign_labels_using_kmeans(alea=i)\n",
    "    labels_kmeans.append(labels)\n",
    "    if i > 0:\n",
    "        # Calculate ARI\n",
    "        ari = adjusted_rand_score(labels_kmeans[i], labels_kmeans[i-1])\n",
    "        print(f'ARI = {ari}', '\\n')\n",
    "\n",
    "display(labels_kmeans)\n",
    "\n",
    "# Tous les k-means se ressemblent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation, premier clustering RFMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ca a l'air super stable.\n",
    "# Visualisons !\n",
    "\n",
    "custom_color_set_5 = ['red', 'pink', 'lightgreen', 'yellow', 'blue']\n",
    "\n",
    "def plot_kmeans_3D(df=rfms_scaled, legend=False):\n",
    "    fig = px.scatter_3d(df, x='recent_timestamp', y='nb_commandes', z='payment_total', color=labels_kmeans[0],\n",
    "                        color_discrete_sequence=custom_color_set_5, opacity=1)\n",
    "\n",
    "    fig.update_traces(marker=dict(size=5), showlegend=False)\n",
    "    fig.update_layout(title='K-Means Clustering in 3D + hue')\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=800,  # Specify the width in pixels\n",
    "        height=600  # Specify the height in pixels\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_kmeans_3D()\n",
    "\n",
    "# la légende est displayed qd mm ?\n",
    "# Couleurs pas ds le bon ordre\n",
    "\n",
    "# Je distingue 4 clusters, mais je ne vois pas bien le 5ème (en jaune)\n",
    "# Ce cluster est peut-être davantage lié à notre 4ème dimension, qui n'est pas représentée ici.\n",
    "\n",
    "# Les 4 clusters visibles rappelent fortement notre segmentation RFM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changeons d'Axes3D\n",
    "\n",
    "cluster_colors = custom_color_set_5[:len(set(labels_kmeans[0]))]\n",
    "rfms_scaled['labels'] = labels_kmeans[0].astype(int).astype(str)\n",
    "\n",
    "fig = px.scatter_3d(rfms_scaled, x='recent_timestamp', y='satisfaction', z='payment_total', color='labels',\n",
    "                            color_discrete_sequence=cluster_colors, opacity=0.9)\n",
    "\n",
    "fig.update_traces(marker=dict(size=5), showlegend=True)\n",
    "fig.update_layout(title='K-Means Clustering in 4D, low resolution')\n",
    "\n",
    "fig.update_layout(\n",
    "    width=800,  # Specify the width in pixels\n",
    "    height=600  # Specify the height in pixels\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La légende ne sert pas à gd chose ici, c + clair sans.\n",
    "# Ou en mode discret (catégorique)\n",
    "# couleurs OK, legend à ordonner\n",
    "\n",
    "def plot_kmeans_3D(df=rfms_scaled, legend=True):\n",
    "    # Convert cluster labels to integer and then to string format\n",
    "    cluster_labels_str = labels_kmeans[0].astype(int).astype(str)\n",
    "    # Create a list of colors based on labels\n",
    "    cluster_colors = custom_color_set_5[:len(set(labels_kmeans[0]))]\n",
    "\n",
    "    # Create a 3D scatter plot with custom legend\n",
    "    fig = px.scatter_3d(df, x='recent_timestamp', y='nb_commandes', z='payment_total', color=cluster_labels_str,\n",
    "                        color_discrete_sequence=cluster_colors, opacity=0.9)\n",
    "\n",
    "    fig.update_traces(marker=dict(size=5), showlegend=legend)\n",
    "    fig.update_layout(title='K-Means Clustering in 3D')\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=800,  # Specify the width in pixels\n",
    "        height=600  # Specify the height in pixels\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_kmeans_3D(legend=True)\n",
    "\n",
    "# On peut tjs essayer de comprendre une partie de notre clustering directement :\n",
    "# le cluster bleu (2) semble tjs regrouper les clients les plus réguliers.\n",
    "\n",
    "# Cependant les autres clusters sont plus difficiles à distinguer, donc\n",
    "# plus difficiles à interpréter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legende tjs désordre\n",
    "\n",
    "def plot_clusters_4D(df=rfms_scaled, legend=True):\n",
    "    subset = []\n",
    "    # Create a list of colors based on labels\n",
    "    cluster_colors = custom_color_set_5[:len(set(labels_kmeans[0]))]\n",
    "\n",
    "    unique_values = np.sort(df['round_sat'].unique())\n",
    "    print(unique_values)\n",
    "\n",
    "    for i, score in enumerate(unique_values):\n",
    "        subset.append(df.loc[df['round_sat'] == score, :].copy())\n",
    "\n",
    "        print(f'Subset review_score moyen == {score} : {subset[i-1].shape[0]} clients')\n",
    "\n",
    "        # Create a 3D scatter plot with custom legend\n",
    "        fig = px.scatter_3d(subset[i-1], x='recent_timestamp', y='nb_commandes', z='payment_total', color='labels',\n",
    "                            color_discrete_sequence=cluster_colors, opacity=0.9)\n",
    "\n",
    "        fig.update_traces(marker=dict(size=5), showlegend=legend)\n",
    "        fig.update_layout(title='K-Means Clustering in 4D, low resolution')\n",
    "\n",
    "        fig.update_layout(\n",
    "            width=600,  # Specify the width in pixels\n",
    "            height=400  # Specify the height in pixels\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "plot_clusters_4D()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nope, c pire...\n",
    "\n",
    "rfms_scaled['color'] = \"\"\n",
    "\n",
    "for i, label in enumerate(rfms_scaled['labels'].unique()):\n",
    "    rfms_scaled.loc[rfms_scaled['labels'] == label, 'color'] = custom_color_set_5[i]\n",
    "\n",
    "def plot_clusters_4D(df=rfms_scaled, legend=True):\n",
    "    subset = []\n",
    "    # Create a list of colors based on labels\n",
    "    cluster_colors = custom_color_set_5[:len(set(labels_kmeans[0]))]\n",
    "\n",
    "    unique_values = np.sort(df['round_sat'].unique())\n",
    "    print(unique_values)\n",
    "\n",
    "    for i, score in enumerate(unique_values):\n",
    "        subset.append(df.loc[df['round_sat'] == score, :].copy())\n",
    "        print(f'Subset review_score moyen == {score} : {subset[i-1].shape[0]} clients')\n",
    "\n",
    "        # Create a 3D scatter plot with custom legend\n",
    "        fig = px.scatter_3d(subset[i-1], x='recent_timestamp', y='nb_commandes', z='payment_total', color='color')\n",
    "\n",
    "        fig.update_traces(marker=dict(size=5), showlegend=legend)\n",
    "        fig.update_layout(title='K-Means Clustering in 4D, low resolution')\n",
    "\n",
    "        fig.update_layout(\n",
    "            width=600,  # Specify the width in pixels\n",
    "            height=400  # Specify the height in pixels\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "plot_clusters_4D()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tjs pas... compliqué les couleurs avec plotly\n",
    "\n",
    "def plot_clusters_4D(df=rfms_scaled, legend=True):\n",
    "    subset = []\n",
    "    # Create a list of colors based on labels\n",
    "    cluster_colors = custom_color_set_5[:len(set(labels_kmeans[0]))]\n",
    "\n",
    "    unique_values = np.sort(df['round_sat'].unique())\n",
    "    print(unique_values)\n",
    "\n",
    "    for i, score in enumerate(unique_values):\n",
    "        subset.append(df.loc[df['round_sat'] == score, :].copy())\n",
    "        subset[i-1].sort_values(by='labels', inplace=True)\n",
    "\n",
    "        print(f'Subset review_score moyen == {score} : {subset[i-1].shape[0]} clients')\n",
    "\n",
    "        # Create a 3D scatter plot with custom legend\n",
    "        fig = px.scatter_3d(subset[i-1], x='recent_timestamp', y='nb_commandes', z='payment_total', color='labels',\n",
    "                            color_discrete_sequence=cluster_colors, opacity=0.9)\n",
    "\n",
    "        fig.update_traces(marker=dict(size=5), showlegend=legend)\n",
    "        fig.update_layout(title='K-Means Clustering in 4D, low resolution')\n",
    "\n",
    "        fig.update_layout(\n",
    "            width=600,  # Specify the width in pixels\n",
    "            height=400  # Specify the height in pixels\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "plot_clusters_4D()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En trichant ?\n",
    "# Enfin des couleurs cohérentes !\n",
    "# Mauve et bleu, et jaune et pourpre.\n",
    "# En fait j'aurais pu conserver la boucle et looper sur une liste de liste de couleurs\n",
    "\n",
    "def plot_clusters_4D(df=rfms_scaled, legend=True):\n",
    "    subset = []\n",
    "    # Create a list of colors based on labels\n",
    "    cluster_colors = custom_color_set_5[:len(set(labels_kmeans[0]))]\n",
    "\n",
    "    unique_values = np.sort(df['round_sat'].unique())\n",
    "    print(unique_values)\n",
    "\n",
    "\n",
    "    # 0\n",
    "    subset.append(df.loc[df['round_sat'] == unique_values[0], :].copy())\n",
    "    subset[0].sort_values(by='labels', inplace=True)\n",
    "\n",
    "    print(f'Subset review_score moyen == {unique_values[0]} : {subset[0].shape[0]} clients')\n",
    "\n",
    "    fig = px.scatter_3d(subset[0], x='recent_timestamp', y='nb_commandes', z='payment_total', color='labels',\n",
    "                        color_discrete_sequence=['yellow', 'lightgreen', 'red'], opacity=0.9)\n",
    "\n",
    "    fig.update_traces(marker=dict(size=5), showlegend=legend)\n",
    "    fig.update_layout(title='K-Means Clustering in 4D, low resolution')\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=600,  # Specify the width in pixels\n",
    "        height=400  # Specify the height in pixels\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # 1\n",
    "    subset.append(df.loc[df['round_sat'] == unique_values[1], :].copy())\n",
    "    subset[1].sort_values(by='labels', inplace=True)\n",
    "\n",
    "    print(f'Subset review_score moyen == {unique_values[1]} : {subset[1].shape[0]} clients')\n",
    "\n",
    "    fig = px.scatter_3d(subset[1], x='recent_timestamp', y='nb_commandes', z='payment_total', color='labels',\n",
    "                        color_discrete_sequence=['orange', 'yellow', 'lightgreen', 'silver', 'red'], opacity=0.9)\n",
    "\n",
    "    fig.update_traces(marker=dict(size=5), showlegend=legend)\n",
    "    fig.update_layout(title='K-Means Clustering in 4D, low resolution')\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=600,  # Specify the width in pixels\n",
    "        height=400  # Specify the height in pixels\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # 2\n",
    "    subset.append(df.loc[df['round_sat'] == unique_values[2], :].copy())\n",
    "    subset[2].sort_values(by='labels', inplace=True)\n",
    "\n",
    "    print(f'Subset review_score moyen == {unique_values[2]} : {subset[2].shape[0]} clients')\n",
    "\n",
    "    # Create a 3D scatter plot with custom legend\n",
    "    fig = px.scatter_3d(subset[2], x='recent_timestamp', y='nb_commandes', z='payment_total', color='labels',\n",
    "                        color_discrete_sequence=['orange', 'yellow', 'lightgreen', 'silver', 'red'], opacity=0.9)\n",
    "\n",
    "    fig.update_traces(marker=dict(size=5), showlegend=legend)\n",
    "    fig.update_layout(title='K-Means Clustering in 4D, low resolution')\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=600,  # Specify the width in pixels\n",
    "        height=400  # Specify the height in pixels\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # 3\n",
    "    subset.append(df.loc[df['round_sat'] == unique_values[3], :].copy())\n",
    "    subset[3].sort_values(by='labels', inplace=True)\n",
    "\n",
    "    print(f'Subset review_score moyen == {unique_values[3]} : {subset[3].shape[0]} clients')\n",
    "\n",
    "    # Create a 3D scatter plot with custom legend\n",
    "    fig = px.scatter_3d(subset[3], x='recent_timestamp', y='nb_commandes', z='payment_total', color='labels',\n",
    "                        color_discrete_sequence=['orange', 'yellow', 'lightgreen', 'silver'], opacity=0.9)\n",
    "\n",
    "    fig.update_traces(marker=dict(size=5), showlegend=legend)\n",
    "    fig.update_layout(title='K-Means Clustering in 4D, low resolution')\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=600,  # Specify the width in pixels\n",
    "        height=400  # Specify the height in pixels\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # 4\n",
    "    subset.append(df.loc[df['round_sat'] == unique_values[4], :].copy())\n",
    "    subset[4].sort_values(by='labels', inplace=True)\n",
    "\n",
    "    print(f'Subset review_score moyen == {unique_values[4]} : {subset[4].shape[0]} clients')\n",
    "\n",
    "    # Create a 3D scatter plot with custom legend\n",
    "    fig = px.scatter_3d(subset[4], x='recent_timestamp', y='nb_commandes', z='payment_total', color='labels',\n",
    "                        color_discrete_sequence=['orange', 'yellow', 'lightgreen', 'silver'], opacity=0.9)\n",
    "\n",
    "\n",
    "    fig.update_traces(marker=dict(size=5), showlegend=legend)\n",
    "    fig.update_layout(title='K-Means Clustering in 4D, low resolution')\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=600,  # Specify the width in pixels\n",
    "        height=400  # Specify the height in pixels\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_clusters_4D()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion : DESCRIPTION ACTIONNABLE du clustering obtenu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On voit enfin tte notre data en 4D !\n",
    "# Cependant, on est très limités au niveau de la \"résolution\"...\n",
    "# J'ai essayé d'itérer sur ttes les valeurs possibles du review_score moyen, 34 valeurs en tout.\n",
    "# print(rfms_scaled['satisfaction'].nunique())\n",
    "\n",
    "# Mais la cellule fait crasher le notebook au-delà de qq plots.\n",
    "# Du coup je continue à utiliser la valeur bucketisée en 5 tranches.\n",
    "\n",
    "# Autre conséquence : on ne pourra mm pas vidualiser la 5eme dim avec un notebook,\n",
    "# c'est vraiment dommage. -> chercher solutions\n",
    "\n",
    "# On voit tt de mm qqch apparaitre ici (malgré les couleurs aui sont à nouveau mélangées...)\n",
    "# ! Utiliser la legende, les couleurs changent à chaque sous-plot !\n",
    "# ! CORRIGER CES FICHUES COULEURS ! ELLES COMPLIQUENT LA LECTURE AU LIEU D'AIDER\n",
    "\n",
    "# Lecture :\n",
    "\n",
    "# Les clusters 0 et 3 regroupent uniquement des clients \"satisfaits\" (review_score moyen ou +),\n",
    "# c'est pourquoi ils n'apparaissent pas sur les 2 premiers plots (review_scores faibles)\n",
    "\n",
    "# Le cluster 3 représente donc les \"pire clients\" (1 seule commande, au montant relativement peu élevé,\n",
    "# il y a longtemps), parmi les clients satisfaits.\n",
    "# = Je les appelerais le groupe des \"y a pas de raison qu'ils recommandent pas, mais bon\".\n",
    "# Pour Olis c'est un groupe assez prometteur de bons clients potentiels, puisqu'ils ont déjà utilisé\n",
    "# la plateforme et se sont déclaré satisfaits, mais sans doute à \"réactiver\" car\n",
    "# la plupart n'ont pas recommandé depuis + d'un an.\n",
    "\n",
    "# Le cluster 0 regroupe des clients qui eux aussi sont satisfaits, n'ont fait qu'une commande,\n",
    "# au montant peu élevé, mais plus récente.\n",
    "# = un groupe très prometteur, en raison surtout de la commande récente + score satisfait\n",
    "\n",
    "# Le cluster 1 regroupe les clients qui ont dépensé le plus, ont fait 1 seule commande,\n",
    "# et sont plutôt satisfaits.\n",
    "# Il semble relativement indépendant de la récence et de la satisfaction\n",
    "# (réparti sur toute la longueur de l'axe, de manière assez homogène)\n",
    "# = Très fort potentiel, clients à cibler car potentiellement dépensiers !\n",
    "\n",
    "# Le cluster 2 regroupe clairement les clients réguliers (2 commandes et +)\n",
    "\n",
    "# Le 4 regroupe des clients très mécontents ou au mieux moyennement satisfaits,\n",
    "# n'ayant fait qu'une seule commande, indépendamment de la récence et du montant.\n",
    "# = sans doute le groupe le moins prometteur.\n",
    "# Cibler ce groupe pour comprendre les causes possibles d'insatisfaction,\n",
    "# et tenter de corriger ds une certaine mesure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 dendro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on a very small sample (10-15s) (très déformé)\n",
    "# dendro_df = rfms_scaled[features_RFMS][::50].copy()\n",
    "\n",
    "# Work on a small sample (1 min 30s)\n",
    "dendro_df = rfms_scaled[features_RFMS][::10].copy()\n",
    "\n",
    "# Use complete data : impossible without modifying notebook params\n",
    "# (dataset way too big for an algo with this order of complexity)\n",
    "\n",
    "display(Image(filename='img/dendro_rfms.png'))\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(dendro_df, method='ward')  # You can choose different linkage methods\n",
    "\n",
    "# Create a dendrogram\n",
    "dendrogram(linkage_matrix, labels=dendro_df.index.values)\n",
    "\n",
    "# Display the dendrogram\n",
    "plt.title(\"Dendrogram\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "plt.axhline(y=82, color='orange', linestyle='--', label='Threshold')\n",
    "# best number of clusters = 4 with this first \"method\"\n",
    "# (More like a quick empirical \"rule of thumb\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Bcp + long que k-means, compliqué pour travailler sur le dataset complet...\n",
    "# Propose 4-5 clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nb optimal de cluster, using inconsistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters using inconsistency\n",
    "depth = 5  # You can adjust this depth parameter\n",
    "inconsistencies = inconsistent(linkage_matrix, depth)\n",
    "optimal_cluster_count = inconsistencies.argmax() + 1\n",
    "\n",
    "# Plot the inconsistency values\n",
    "plt.figure()\n",
    "plt.plot(range(2, len(inconsistencies) + 2), inconsistencies)\n",
    "plt.title(\"Inconsistency vs. Number of Clusters\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Inconsistency\")\n",
    "plt.axvline(x=optimal_cluster_count, color='red', linestyle='--', label='Optimal Cluster Count')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 38 000 clusters, c peut-être mathématiquement optimal, mais...\n",
    "# Bon courage pour l'interprétation !\n",
    "\n",
    "# Plot the inconsistency values for up to 15 clusters\n",
    "max_clusters_to_plot = 15\n",
    "optimal_cluster_count = inconsistencies[:max_clusters_to_plot - 1].argmax() + 2 # index starts at 2\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(range(2, max_clusters_to_plot + 2), inconsistencies[:max_clusters_to_plot])\n",
    "\n",
    "plt.title(\"Inconsistency vs. Number of Clusters\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Inconsistency\")\n",
    "plt.axvline(x=optimal_cluster_count, color='red', linestyle='--', label='Optimal Cluster Count')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# On retrouve nos 4 clusters\n",
    "# or do we ?\n",
    "\n",
    "optimal_cluster_count = 5\n",
    "\n",
    "# Extract cluster labels\n",
    "cluster_labels_dendro_5 = fcluster(linkage_matrix, t=optimal_cluster_count, criterion='maxclust')\n",
    "\n",
    "# Compare dendro clusters to kmeans clusters\n",
    "labels_kmeans_sample = assign_labels_using_kmeans(alea=42, df=dendro_df, features=features_RFMS, best_k=5, silhouette=True)\n",
    "ari = adjusted_rand_score(labels_kmeans_sample, cluster_labels_dendro_5)\n",
    "\n",
    "print(f'ARI = {ari}', '\\n')\n",
    "\n",
    "# Silhouette kmeans = 0.32\n",
    "# ARI = 0.6, les 2 partitionnements semblent assez différents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualité des clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enfin, évaluons la qualité des clusters obtenus\n",
    "\n",
    "def evaluate_dendro_clusters(linkage='ward'):\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for n_clusters in range(2, 10):\n",
    "        # Create an AgglomerativeClustering model with the chosen linkage method and the current number of clusters\n",
    "        model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
    "\n",
    "        # Fit the model to your data\n",
    "        cluster_labels = model.fit_predict(dendro_df)\n",
    "\n",
    "        # Calculate the silhouette score for the current clustering\n",
    "        silhouette_avg = silhouette_score(dendro_df, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Plot the silhouette scores\n",
    "    plt.figure()\n",
    "    plt.plot(range(2, 10), silhouette_scores, marker='o', linestyle='-', color='b')\n",
    "    plt.title(\"Silhouette Score vs. Number of Clusters\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Find the optimal number of clusters (highest silhouette score)\n",
    "    optimal_clusters = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "    plt.axvline(x=optimal_clusters, color='r', linestyle='--', label=f'Optimal Clusters ({optimal_clusters})')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "evaluate_dendro_clusters()\n",
    "\n",
    "# Pour nb clusters > 2, on retrouve encore 5 comme meilleure valeur.\n",
    "# Le silhouette score du kmeans était meilleur.\n",
    "# Tant mieux, l'algo le + rapide est aussi le + performant ici.\n",
    "# En + kmeans peut travailler avec tt le dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testons d'autres méthodes\n",
    "\n",
    "evaluate_dendro_clusters(linkage='single')\n",
    "\n",
    "# ??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dendro_clusters(linkage='complete')\n",
    "\n",
    "# le silhouette score est curieux ici aussi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dendro_clusters(linkage='average')\n",
    "\n",
    "# c quoi ces silhouettes ??\n",
    "# Dues à une perte de complexité lors du sampling ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le kmeans l'emporte encore.\n",
    "\n",
    "# - pour la qualité supérieure de son clustering,\n",
    "\n",
    "# - pour sa capacité à travailler sur le jeu de données entier, rapidement.\n",
    "# En effet la \"limitation\" du clustering hiérarchique (sa complexité algorithmique lourde)\n",
    "# est un problème sur ce dataset important.\n",
    "\n",
    "# Notons cependant que pour l'étape finale, le modèle choisi ne travaillera plus que sur une tranche\n",
    "# réduite du dataset, mise à jour régulièrement. Le clustering hiérarchique reste donc une possibilité.\n",
    "# (Ceci dit, les scores du k-means restent meilleurs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"à la main\"\n",
    "\n",
    "# (ajouter des couleurs)\n",
    "\n",
    "def plot_DBSCAN_3D(df=rfms_scaled[features_RFMS], legend=True, color_set=custom_color_set_5, opacity=1.0):\n",
    "    # Specify the DBSCAN parameters (eps and min_samples)\n",
    "    eps = 0.2  # Maximum distance to form a dense cluster\n",
    "    min_samples = 5  # Minimum number of samples in a neighborhood to be considered a core point\n",
    "\n",
    "    # Initialize the DBSCAN clustering model\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\n",
    "    # Fit the DBSCAN model to the data\n",
    "    dbscan.fit(df)\n",
    "\n",
    "    # Extract cluster labels (-1 represents noise points)\n",
    "    labels = dbscan.labels_\n",
    "    print(f'DBSCAN propose {len(set(labels))} clusters.')\n",
    "\n",
    "    # Convert cluster labels to integer and then to string format\n",
    "    cluster_labels_str = labels.astype(int).astype(str)\n",
    "\n",
    "    # Create a list of colors based on labels\n",
    "    if color_set is None:\n",
    "        cluster_colors = custom_color_set_5[:len(set(labels))]\n",
    "    else:\n",
    "        cluster_colors = color_set[:len(set(labels))]\n",
    "\n",
    "    # Create a 3D scatter plot with custom legend\n",
    "    fig = px.scatter_3d(df, x='recent_timestamp', y='nb_commandes', z='payment_total', color=cluster_labels_str,\n",
    "                        color_discrete_sequence=cluster_colors, opacity=opacity)\n",
    "\n",
    "    fig.update_traces(marker=dict(size=5), showlegend=legend)\n",
    "    fig.update_layout(title='DBSCAN Clustering in 3D')\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=800,  # Specify the width in pixels\n",
    "        height=600  # Specify the height in pixels\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_DBSCAN_3D()\n",
    "\n",
    "# mm ?\n",
    "\n",
    "# On passe toujours très vide de 1 cluster à une multide de clusters,\n",
    "# pour des valeurs de eps assez proches.\n",
    "# Difficile d'obtenir juste qq clusters qui aient du sens métier.\n",
    "\n",
    "# MM conclusion qu'en 3 dimensions :\n",
    "# Nos points sont espacés de manière trop régulière pour le DBSCAN,\n",
    "# la topologie n'est pas intéressante pour cet algo car\n",
    "# Il n'y a pas de zone de vide qui sépare naturellement nos données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avec un genre de gridsearchcv\n",
    "\n",
    "# Turns out, GridSearchCV has no score really suitable for clustering evaluation\n",
    "# We need to do a \"manual gridsearch\"\n",
    "\n",
    "# Define the range of eps and min_samples values\n",
    "eps_values = [0.1, 0.2, 0.3]\n",
    "min_samples_values = [3, 5, 7]\n",
    "\n",
    "# Initialize lists to store scores and cluster counts\n",
    "dbscan_scores = []\n",
    "cluster_counts = []\n",
    "\n",
    "# Perform manual grid search\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(rfms_scaled[features_RFMS])\n",
    "        # Silhouette prend bcp + de tps\n",
    "        # score = silhouette_score(rfm_log_scaled, labels)\n",
    "        # Utilisons D-B ici :\n",
    "        score = davies_bouldin_score(rfms_scaled[features_RFMS], labels)\n",
    "        dbscan_scores.append(score)\n",
    "        nb_clusters = len(set(labels))\n",
    "        cluster_counts.append(nb_clusters)\n",
    "\n",
    "print('On trouve entre ' + str(min(cluster_counts)) + ' clusters (valeurs élevées pour eps et min_samples)')\n",
    "print('et ' + str(max(cluster_counts)) + ' clusters (valeurs faibles)')\n",
    "\n",
    "# Problème : qd le nb de clusters diminue, le score de Davies-Bouldin augmente,\n",
    "# ce qui est mauvais : cela signifie des clusters moins compacts et/ou moins séparés.\n",
    "# Il y a donc un compromis à faire entre la qualité du clustering et son utilité métier\n",
    "\n",
    "# Or on peut vérifier que pour un nb de clusters donné le kmeans obtenait un score bien inférieur (donc meilleur)\n",
    "# Encore une fois, sur ces données, le kmeans est à la fois + rapide et plus performant\n",
    "\n",
    "# Reshape the scores and cluster counts for plotting\n",
    "dbscan_scores = np.array(dbscan_scores).reshape(len(eps_values), len(min_samples_values))\n",
    "cluster_counts = np.array(cluster_counts).reshape(len(eps_values), len(min_samples_values))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(dbscan_scores, interpolation='nearest', cmap=plt.cm.autumn_r)\n",
    "plt.title(\"DB Scores\")\n",
    "plt.xticks(np.arange(len(min_samples_values)), min_samples_values)\n",
    "plt.yticks(np.arange(len(eps_values)), eps_values)\n",
    "plt.xlabel(\"min_samples\")\n",
    "plt.ylabel(\"eps\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cluster_counts, interpolation='nearest', cmap=plt.cm.autumn_r)\n",
    "plt.title(\"Number of Clusters\")\n",
    "plt.xticks(np.arange(len(min_samples_values)), min_samples_values)\n",
    "plt.yticks(np.arange(len(eps_values)), eps_values)\n",
    "plt.xlabel(\"min_samples\")\n",
    "plt.ylabel(\"eps\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom gradual cmap,\n",
    "# higher parameter values\n",
    "\n",
    "color1 = (1.0, 1.0, 0.3)\n",
    "color2 = (1.0, 0.1, 0.1)\n",
    "\n",
    "num_segments = 256\n",
    "\n",
    "gradient_colors = [color1, color2]\n",
    "gradual_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", gradient_colors, N=num_segments)\n",
    "\n",
    "eps_values = [0.2, 0.3, 0.4]\n",
    "min_samples_values = [5, 7, 9]\n",
    "\n",
    "dbscan_scores = []\n",
    "cluster_counts = []\n",
    "\n",
    "# Perform manual grid search\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(rfms_scaled[features_RFMS])\n",
    "        score = davies_bouldin_score(rfms_scaled[features_RFMS], labels)\n",
    "        dbscan_scores.append(score)\n",
    "        nb_clusters = len(set(labels))\n",
    "        cluster_counts.append(nb_clusters)\n",
    "\n",
    "print('On trouve entre ' + str(min(cluster_counts)) + ' clusters (valeurs élevées pour eps et min_samples)')\n",
    "print('et ' + str(max(cluster_counts)) + ' clusters (valeurs faibles)')\n",
    "\n",
    "dbscan_scores = np.array(dbscan_scores).reshape(len(eps_values), len(min_samples_values))\n",
    "cluster_counts = np.array(cluster_counts).reshape(len(eps_values), len(min_samples_values))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(dbscan_scores, interpolation='nearest', cmap=gradual_cmap)\n",
    "plt.title(\"DB Scores\")\n",
    "plt.xticks(np.arange(len(min_samples_values)), min_samples_values)\n",
    "plt.yticks(np.arange(len(eps_values)), eps_values)\n",
    "plt.xlabel(\"min_samples\")\n",
    "plt.ylabel(\"eps\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cluster_counts, interpolation='nearest', cmap=gradual_cmap)\n",
    "plt.title(\"Number of Clusters\")\n",
    "plt.xticks(np.arange(len(min_samples_values)), min_samples_values)\n",
    "plt.yticks(np.arange(len(eps_values)), eps_values)\n",
    "plt.xlabel(\"min_samples\")\n",
    "plt.ylabel(\"eps\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# mm conclusion que précédemment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion features RFMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Au final mon idée de couper le dataset en 2 (clients réguliers vs 1 seule commande)\n",
    "# n'a pas été utile, puisque les outils de clustering repèrent facilement cette disposition des données,\n",
    "# et en rendent compte dans l'attribution des labels (= l'assignation à un cluster)\n",
    "\n",
    "# Lorsqu'on prend en compte uniquement ces 4 features,\n",
    "\n",
    "# le nombre optimal de clusters que nous pouvons faire est de 5 clusters,\n",
    "# ce qui mieux. Assez pour une segmentation de qualité ?\n",
    "# Plutôt que de \"forcer\" un sous-partitionnement sub-optimal, nous pourrions ajouter une cinquième feature,\n",
    "# toujours la plus pertinente possible pour comprendre nos clients.\n",
    "\n",
    "# Cependant nous sommes à la limite de nos outils de visualisation, il serait difficile de travailler\n",
    "# en 5 dim dans un notebook avec mes connaissances actuelles.\n",
    "# -> autre outil ? Possibilités de contourner le problème ??\n",
    "\n",
    "# Quelle autre feature aurions-nous choisi ?\n",
    "# Une feature géographique, pour mettre en lumière des inégalités dans l'implantation sur le territoire ?\n",
    "# Je trouve les autres features peut-être moins pertinentes que RFMS pour une entreprise comme Olis.\n",
    "# Après il faudrait vérifier, c juste une hypothèse.\n",
    "\n",
    "# Sur 4 dimensions comme sur 3, le k-means n'a pas juste été \"plus performant\" que les 2 autres algos ;\n",
    "# il les a littéralement laissés dans la poussière ! (/ mis KO)\n",
    "# car mm en 4 dimensions, notre data est répartie de manière relativement homogène,\n",
    "# sa topologie est idéale pour le k-means (et vice-versa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annexes / tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian mixture models ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le t-SNE ne permet pas d'interpréter les distances ou les tailles des clusters sur le graph obtenu,\n",
    "# ce n'est donc pas du tout l'outil idéal pour une segmentation de clientèle.\n",
    "# C juste pour l'essayer !\n",
    "\n",
    "# Perform t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "tsne_result = tsne.fit_transform(rfms_scaled[features_RFMS])\n",
    "\n",
    "# Create a DataFrame to hold the t-SNE results\n",
    "df_tsne = pd.DataFrame(tsne_result, columns=[\"Dimension 1\", \"Dimension 2\"])\n",
    "\n",
    "# Visualize the t-SNE results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df_tsne[\"Dimension 1\"], df_tsne[\"Dimension 2\"])\n",
    "plt.title(\"t-SNE Visualization\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.show()\n",
    "\n",
    "# Wow ! De toute beauté !\n",
    "\n",
    "# Après recherche, j'avais mal compris. Le t-SNE peut aider dans certains cas à détecter des clusters,\n",
    "# mais ce n'est pas un outil de clusterisation à proprement parler. Il n'assigne pas de labels.\n",
    "# C'est un outil de réduction dimentionnelle, et pour l'instant on n'en a mm pas besoin,\n",
    "# on est \"seulement\" en 3D et on n'ira pas en dimensions très élevées.\n",
    "\n",
    "# Serait-ce notre groupe des 3% qu'on devine à droite ?\n",
    "# Peut-être. Aucune idée.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE with 3 components\n",
    "tsne = TSNE(n_components=3, perplexity=30, n_iter=300)\n",
    "tsne_result = tsne.fit_transform(rfms_scaled[features_RFMS])\n",
    "\n",
    "# Create a DataFrame to hold the t-SNE results\n",
    "df_tsne = pd.DataFrame(tsne_result, columns=[\"Dimension 1\", \"Dimension 2\", \"Dimension 3\"])\n",
    "\n",
    "# Visualize the t-SNE results in 3D\n",
    "fig = px.scatter_3d(df_tsne, x=\"Dimension 1\", y=\"Dimension 2\", z=\"Dimension 3\")\n",
    "fig.update_layout(title=\"t-SNE Visualization (3D)\")\n",
    "fig.show()\n",
    "\n",
    "# La Terre est vraiment bleue comme une orange !\n",
    "# La preuve :\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_p4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
