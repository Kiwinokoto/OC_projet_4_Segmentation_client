{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Segmentez des clients d'un site e-commerce**\n",
    "\n",
    "## partie 2/4 : essais des différentes approches de modélisation\n",
    "\n",
    "### <br> Résumé des épisodes précédents :\n",
    "\n",
    "> &emsp; La data fournie par Olis est plutôt propre et bien organisée. Une analyse rapide révèle cependant <br>deux choses qui semblent plutôt contradictoires :<br><br> > &emsp; D'un côté, le nombre de transactions mensuelles traitées sur la période augmente régulièrement,\n",
    "> tout comme le volume des ventes. En arrondissant, ils doublent, puis triplent rapidement (en moins de deux ans, cette croissance est exceptionnelle.) <br>\n",
    "> Le score moyen de satisfaction des clients, > 4/5, est aussi très bon. <br><br> > &emsp; D'un autre côté... 97 % des clients n'ont jamais fait de deuxième achat. En 2 ans. <br><br> > &emsp; Etonnant, non ?? <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Importation des librairies, réglages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import SpanSelector\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, inconsistent\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from IPython.display import Image\n",
    "# from zipfile import ZipFile\n",
    "\n",
    "print('Python version ' + sys.version)\n",
    "print('\\npandas version ' + pd.__version__)\n",
    "print('sns version ' + sns.__version__)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', 200)\n",
    "sns.set(font_scale=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_look(df, miss=True):\n",
    "    \"\"\"\n",
    "    Display a quick overview of a DataFrame, including shape, head, tail, unique values, and duplicates.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to inspect.\n",
    "        check_missing (bool, optional): Whether to check and display missing values (default is True).\n",
    "\n",
    "    The function provides a summary of the DataFrame, including its shape, the first and last rows, the count of unique values per column, and the number of duplicates.\n",
    "    If `check_missing` is set to True, it also displays missing value information.\n",
    "    \"\"\"\n",
    "    print(f'shape : {df.shape}')\n",
    "\n",
    "    display(df.head())\n",
    "    display(df.tail())\n",
    "\n",
    "    print('uniques :')\n",
    "    display(df.nunique())\n",
    "\n",
    "    print('Doublons ? ', df.duplicated(keep='first').sum(), '\\n')\n",
    "\n",
    "    if miss:\n",
    "        display(get_missing_values(df))\n",
    "\n",
    "\n",
    "def lerp(a, b, t):\n",
    "    \"\"\"\n",
    "    Linear interpolation between two values 'a' and 'b' at a parameter 't'.\n",
    "    A very useful little function, used here to position annotations in plots.\n",
    "    Got it coding with Radu :)\n",
    "\n",
    "    Given two values 'a' and 'b', and a parameter 't',\n",
    "    this function calculates the linear interpolation between 'a' and 'b' at 't'.\n",
    "\n",
    "    Parameters:\n",
    "    a (float or int): The start value.\n",
    "    b (float or int): The end value.\n",
    "    t (float): The interpolation parameter (typically in the range [0, 1], but can be outside).\n",
    "\n",
    "    Returns:\n",
    "    float or int: The interpolated value at parameter 't'.\n",
    "    \"\"\"\n",
    "    return a + (b - a) * t\n",
    "\n",
    "\n",
    "def generate_random_pastel_colors(n):\n",
    "    \"\"\"\n",
    "    Generates a list of n random pastel colors, represented as RGBA tuples.\n",
    "\n",
    "    Parameters:\n",
    "    n (int): The number of pastel colors to generate.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of RGBA tuples representing random pastel colors.\n",
    "\n",
    "    Example:\n",
    "    >>> generate_random_pastel_colors(2)\n",
    "    [(0.749, 0.827, 0.886, 1.0), (0.886, 0.749, 0.827, 1.0)]\n",
    "    \"\"\"\n",
    "    colors = []\n",
    "    for _ in range(n):\n",
    "        # Generate random pastels\n",
    "        red = round(random.randint(150, 250) / 255.0, 3)\n",
    "        green = round(random.randint(150, 250) / 255.0, 3)\n",
    "        blue = round(random.randint(150, 250) / 255.0, 3)\n",
    "\n",
    "        # Create an RGB color tuple and add it to the list\n",
    "        color = (red,green,blue, 1.0)\n",
    "        colors.append(color)\n",
    "\n",
    "    return colors\n",
    "\n",
    "print(generate_random_pastel_colors(2))\n",
    "\n",
    "\n",
    "def get_missing_values(df):\n",
    "    \"\"\"Generates a DataFrame containing the count and proportion of missing values for each feature.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with columns for the feature name, count of missing values,\n",
    "        count of non-missing values, proportion of missing values, and data type for each feature.\n",
    "    \"\"\"\n",
    "    # Count the missing values for each column\n",
    "    missing = df.isna().sum()\n",
    "\n",
    "    # Calculate the percentage of missing values\n",
    "    percent_missing = df.isna().mean() * 100\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    missings_df = pd.DataFrame({\n",
    "        'column_name': df.columns,\n",
    "        'missing': missing,\n",
    "        'present': df.shape[0] - missing,  # Count of non-missing values\n",
    "        'percent_missing': percent_missing.round(2),  # Rounded to 2 decimal places\n",
    "        'type': df.dtypes\n",
    "    })\n",
    "\n",
    "    # Sort the DataFrame by the count of missing values\n",
    "    missings_df.sort_values('missing', inplace=True)\n",
    "\n",
    "    return missings_df\n",
    "\n",
    "# with pd.option_context('display.max_rows', 1000):\n",
    "#   display(get_missing_values(df))\n",
    "\n",
    "\n",
    "# ma fonction d'origine (non cleanée)\n",
    "def hist_distrib(dataframe, feature, bins, r, density=True):\n",
    "    \"\"\"\n",
    "    Affiche un histogramme, pour visualiser la distribution empirique d'une variable\n",
    "    Argument : df, feature num\n",
    "    \"\"\"\n",
    "    # calcul des tendances centrales :\n",
    "    mode =  str(round(dataframe[feature].mode()[0], r))\n",
    "    # mode is often zero, so Check if there are non nul values in the column\n",
    "    if (dataframe[feature] != 0).any():\n",
    "        mode_non_nul = str(round(dataframe.loc[dataframe[feature] != 0, feature].mode()[0], r))\n",
    "    else:\n",
    "        mode_non_nul = \"N/A\"\n",
    "    mediane = str(round(dataframe[feature].median(), r))\n",
    "    moyenne = str(round(dataframe[feature].mean(), r))\n",
    "    # dispersion :\n",
    "    var_emp = str(round(dataframe[feature].var(ddof=0), r))\n",
    "    coeff_var =  str(round(dataframe[feature].std(ddof=0), r)) # = écart-type empirique / moyenne\n",
    "    # forme\n",
    "    skewness = str(round(dataframe[feature].skew(), 2))\n",
    "    kurtosis = str(round(dataframe[feature].kurtosis(), 2))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    dataframe[feature].hist(density=density, bins=bins, ax=ax)\n",
    "    yt = plt.yticks()\n",
    "    y = lerp(yt[0][0], yt[0][-1], 0.8)\n",
    "    t = y/20\n",
    "    xt = plt.xticks()\n",
    "    x = lerp(xt[0][0], xt[0][-1], 0.7)\n",
    "    plt.title(feature, pad=20, fontsize=18)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    fs =13\n",
    "    plt.annotate('Mode : ' + mode, xy = (x, y), fontsize = fs, xytext = (x, y), color = 'g')\n",
    "    plt.annotate('Mode + : ' + mode_non_nul, xy = (x, y-t), fontsize = fs, xytext = (x, y-t), color = 'g')\n",
    "    plt.annotate('Médiane : ' + mediane, xy = (x, y-2*t), fontsize = fs, xytext = (x, y-2*t), color = 'g')\n",
    "    plt.annotate('Moyenne : ' + moyenne, xy = (x, y-3*t), fontsize = fs, xytext = (x, y-3*t), color = 'g')\n",
    "\n",
    "    plt.annotate('Var emp : ' + var_emp, xy = (x, y-5*t), fontsize = fs, xytext = (x, y-5*t), color = 'g')\n",
    "    plt.annotate('Coeff var : ' + coeff_var, xy = (x, y-6*t), fontsize = fs, xytext = (x, y-6*t), color = 'g')\n",
    "\n",
    "    plt.annotate('Skewness : ' + skewness, xy = (x, y-8*t), fontsize = fs, xytext = (x, y-8*t), color = 'g')\n",
    "    plt.annotate('Kurtosis : ' + kurtosis, xy = (x, y-9*t), fontsize = fs, xytext = (x, y-9*t), color = 'g')\n",
    "    plt.show()\n",
    "\n",
    "    return float(skewness) # pour eventuel passage au log\n",
    "\n",
    "# version cleanée\n",
    "def hist_distrib(dataframe, feature, bins, decimal_places, density=True):\n",
    "    \"\"\"\n",
    "    Visualize the empirical distribution of a numerical feature using a histogram.\n",
    "    Calcul des principaux indicateurs de tendance centrale, dispersion et forme.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): The input DataFrame containing the feature.\n",
    "        feature (str): The name of the numerical feature to visualize.\n",
    "        bins (int): The number of bins for the histogram.\n",
    "        decimal_places (int): The number of decimal places for rounding numeric values.\n",
    "        density (bool, optional): Whether to display the histogram as a density plot (default is True).\n",
    "\n",
    "    Returns:\n",
    "        float: The skewness of the feature's distribution.\n",
    "\n",
    "    The function generates a histogram of the feature, displays various statistics, and returns the skewness of the distribution.\n",
    "    \"\"\"\n",
    "    # Calculate central tendencies and dispersion\n",
    "    mode_value = round(dataframe[feature].mode()[0], decimal_places)\n",
    "    mode_non_zero = \"N/A\"\n",
    "    if (dataframe[feature] != 0).any():\n",
    "        mode_non_zero = round(dataframe.loc[dataframe[feature] != 0, feature].mode()[0], decimal_places)\n",
    "    median_value = round(dataframe[feature].median(), decimal_places)\n",
    "    mean_value = round(dataframe[feature].mean(), decimal_places)\n",
    "\n",
    "    # Calculate dispersion\n",
    "    var_emp = round(dataframe[feature].var(ddof=0), decimal_places)\n",
    "    coeff_var = round(dataframe[feature].std(ddof=0), decimal_places)\n",
    "\n",
    "    # Calculate shape indicators\n",
    "    skewness_value = round(dataframe[feature].skew(), 2)\n",
    "    kurtosis_value = round(dataframe[feature].kurtosis(), 2)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    dataframe[feature].hist(density=density, bins=bins, ax=ax)\n",
    "\n",
    "    # Adjust placement for annotations\n",
    "    yt = plt.yticks()\n",
    "    y_position = lerp(yt[0][0], yt[0][-1], 0.8)\n",
    "    y_increment = y_position / 20\n",
    "    xt = plt.xticks()\n",
    "    x_position = lerp(xt[0][0], xt[0][-1], 0.7)\n",
    "\n",
    "    # Add annotations with horizontal and vertical alignment\n",
    "    annotation_fs = 13\n",
    "    color = 'g'\n",
    "    ax.annotate(f'Mode: {mode_value}', xy=(x_position, y_position), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position), color=color, ha='left', va='bottom')\n",
    "    ax.annotate(f'Mode +: {mode_non_zero}', xy=(x_position, y_position - y_increment), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position - y_increment), color=color, ha='left', va='bottom')\n",
    "    ax.annotate(f'Median: {median_value}', xy=(x_position, y_position - 2 * y_increment), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position - 2 * y_increment), color=color, ha='left', va='bottom')\n",
    "    ax.annotate(f'Mean: {mean_value}', xy=(x_position, y_position - 3 * y_increment), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position - 3 * y_increment), color=color, ha='left', va='bottom')\n",
    "    ax.annotate(f'Var Emp: {var_emp}', xy=(x_position, y_position - 5 * y_increment), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position - 5 * y_increment), color=color, ha='left', va='bottom')\n",
    "    ax.annotate(f'Coeff Var: {coeff_var}', xy=(x_position, y_position - 6 * y_increment), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position - 6 * y_increment), color=color, ha='left', va='bottom')\n",
    "    ax.annotate(f'Skewness: {skewness_value}', xy=(x_position, y_position - 8 * y_increment), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position - 8 * y_increment), color=color, ha='left', va='bottom')\n",
    "    ax.annotate(f'Kurtosis: {kurtosis_value}', xy=(x_position, y_position - 9 * y_increment), fontsize=annotation_fs,\n",
    "                xytext=(x_position, y_position - 9 * y_increment), color=color, ha='left', va='bottom')\n",
    "\n",
    "    # Label the x-axis and y-axis\n",
    "    ax.set_xlabel(feature, fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.title(f'Distribution of {feature}', pad=20, fontsize=18)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    return skewness_value\n",
    "\n",
    "\n",
    "def boxplot_distrib(dataframe, feature):\n",
    "    \"\"\"\n",
    "    Affiche un boxplot, pour visualiser les tendances centrales et la dispersion d'une variable.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): The input DataFrame containing the feature.\n",
    "        feature (str): The name of the numerical feature to visualize.\n",
    "\n",
    "    The function generates a box plot of the feature to display central tendencies (median and mean) and dispersion.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "    medianprops = {'color':\"blue\"}\n",
    "    meanprops = {'marker':'o', 'markeredgecolor':'black',\n",
    "            'markerfacecolor':'firebrick'}\n",
    "\n",
    "    dataframe.boxplot(feature, vert=False, showfliers=False, medianprops=medianprops, patch_artist=True, showmeans=True, meanprops=meanprops)\n",
    "\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def courbe_lorenz(dataframe, feature):\n",
    "    \"\"\"\n",
    "    Affiche une courbe de Lorenz, pour visualiser la concentration d'une variable\n",
    "    Calcule l'indice de Gini\n",
    "    Visualize a Lorenz curve to assess the concentration of a variable and calculate the Gini coefficient.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): The input DataFrame containing the feature.\n",
    "        feature (str): The name of the numerical feature to visualize.\n",
    "\n",
    "    The function generates a Lorenz curve to assess the concentration of the feature and calculates the Gini coefficient.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    values = dataframe.loc[dataframe[feature].notna(), feature].values\n",
    "    # print(values)\n",
    "    n = len(values)\n",
    "    lorenz = np.cumsum(np.sort(values)) / values.sum()\n",
    "    lorenz = np.append([0],lorenz) # La courbe de Lorenz commence à 0\n",
    "\n",
    "    xaxis = np.linspace(0-1/n,1+1/n,n+1)\n",
    "    #Il y a un segment de taille n pour chaque individu, plus 1 segment supplémentaire d'ordonnée 0.\n",
    "    # #Le premier segment commence à 0-1/n, et le dernier termine à 1+1/n.\n",
    "    plt.plot(xaxis,lorenz,drawstyle='steps-post')\n",
    "    plt.plot(np.arange(2),[x for x in np.arange(2)])\n",
    "    # calcul de l'indice de Gini\n",
    "    AUC = (lorenz.sum() -lorenz[-1]/2 -lorenz[0]/2)/n # Surface sous la courbe de Lorenz. Le premier segment (lorenz[0]) est à moitié en dessous de 0, on le coupe donc en 2, on fait de même pour le dernier segment lorenz[-1] qui est à moitié au dessus de 1.\n",
    "    S = 0.5 - AUC # surface entre la première bissectrice et le courbe de Lorenz\n",
    "    gini = 2*S\n",
    "    plt.annotate('gini =  ' + str(round(gini, 2)), xy = (0.04, 0.88), fontsize = 13, xytext = (0.04, 0.88), color = 'g')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def graphs_analyse_uni(dataframe, feature, bins=50, r=5, density=True):\n",
    "    \"\"\"\n",
    "    Affiche histogramme + boxplot + courbe de Lorenz\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): The input DataFrame containing the feature.\n",
    "        feature (str): The name of the numerical feature to analyze.\n",
    "        bins (int, optional): The number of bins for the histogram (default is 50).\n",
    "        decimal_places (int, optional): The number of decimal places for rounding numeric values (default is 5).\n",
    "        density (bool, optional): Whether to display the histogram as a density plot (default is True).\n",
    "\n",
    "    The function generates and displays an analysis of the given numerical feature, including an histogram, a box plot, and a Lorenz curve.\n",
    "    \"\"\"\n",
    "    hist_distrib(dataframe, feature, bins, r)\n",
    "    boxplot_distrib(dataframe, feature)\n",
    "    courbe_lorenz(dataframe, feature)\n",
    "\n",
    "\n",
    "def shape_head(df, nb_rows=5):\n",
    "    \"\"\"\n",
    "    Affiche les dimensions et les premières lignes dùun dataframe\n",
    "    Display the dimensions and the first rows of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to display.\n",
    "        nb_rows (int, optional): The number of rows to display (default is 5, max is 60).\n",
    "\n",
    "    The function prints the dimensions of the DataFrame and displays the first few rows.\n",
    "    \"\"\"\n",
    "    print(df.shape)\n",
    "    display(df.head(nb_rows))\n",
    "\n",
    "\n",
    "def doughnut(df, feature, title, width=10, height=10):\n",
    "    \"\"\"\n",
    "    Affiche la répartition d'une feature sous forme de diagramme circulaire\n",
    "    Display the distribution of a feature as a doughnut chart.\n",
    "    Les couleurs sont aléatoires.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame containing the feature.\n",
    "        feature (str): The name of the feature to visualize.\n",
    "        title (str): The title for the doughnut chart.\n",
    "        width (int, optional): The width of the chart (default is 10).\n",
    "        height (int, optional): The height of the chart (default is 10).\n",
    "\n",
    "    The function creates a doughnut chart to visualize the distribution of the specified feature.\n",
    "    If you don't like the colors, try running it again :)\n",
    "    \"\"\"\n",
    "    colors = generate_random_pastel_colors(20)\n",
    "\n",
    "    grouped_df = df.groupby(feature).size().to_frame(\"count_per_type\").reset_index()\n",
    "    pie = grouped_df.set_index(feature).copy()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width, height))\n",
    "\n",
    "    patches, texts, autotexts = plt.pie(x=pie['count_per_type'], autopct='%1.1f%%',\n",
    "        startangle=-30, labels=pie.index, textprops={'fontsize':11, 'color':'#000'},\n",
    "        labeldistance=1.25, pctdistance=0.85, colors=colors)\n",
    "\n",
    "    plt.title(\n",
    "    label=title,\n",
    "    fontdict={\"fontsize\":17},\n",
    "    pad=20\n",
    "    )\n",
    "\n",
    "    for text in texts:\n",
    "        # text.set_fontweight('bold')\n",
    "        text.set_horizontalalignment('center')\n",
    "\n",
    "    # Customize percent labels\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_horizontalalignment('center')\n",
    "        autotext.set_fontstyle('italic')\n",
    "        autotext.set_fontsize('10')\n",
    "\n",
    "    #draw circle\n",
    "    centre_circle = plt.Circle((0,0),0.7,fc='white')\n",
    "    fig = plt.gcf()\n",
    "    fig.gca().add_artist(centre_circle)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_non_null_values(df):\n",
    "    \"\"\"\n",
    "    Génère un dataframe contenant le nombre et la proportion de non-null (non-zero) valeurs pour chaque feature\n",
    "    Generate a DataFrame containing the count and proportion of non-null (non-zero) values for each feature.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to analyze.\n",
    "\n",
    "    The function calculates and returns a DataFrame with the count and percentage of non-null (non-zero) values for each feature.\n",
    "    \"\"\"\n",
    "    non_null_counts = df.ne(0).sum()\n",
    "    percent_non_null = (non_null_counts / df.shape[0]) * 100\n",
    "    non_null_values_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                       'non_null_count': non_null_counts,\n",
    "                                       'percent_non_null': percent_non_null.round(2),\n",
    "                                       'type': df.dtypes})\n",
    "    non_null_values_df.sort_values('non_null_count', inplace=True)\n",
    "    return non_null_values_df\n",
    "\n",
    "\n",
    "def get_colors(n=7):\n",
    "    \"\"\"\n",
    "    Generate a list of random colors from multiple colormaps.\n",
    "\n",
    "    Args:\n",
    "        n (int, optional): The number of colors to sample from each colormap (default is 7).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of random colors sampled from different colormaps.\n",
    "    \"\"\"\n",
    "    num_colors_per_colormap = n\n",
    "    colormaps = [plt.cm.Pastel2, plt.cm.Set1, plt.cm.Paired]\n",
    "    all_colors = []\n",
    "\n",
    "    for colormap in colormaps:\n",
    "        colors = colormap(np.linspace(0, 1, num_colors_per_colormap))\n",
    "        all_colors.extend(colors)\n",
    "\n",
    "    np.random.shuffle(all_colors)\n",
    "\n",
    "    return all_colors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_complet = pd.read_csv('data/rfm.csv', sep=',')\n",
    "\n",
    "quick_look(rfm_complet, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm = rfm_complet[['customer_unique_id', 'payment_total', 'most_recent_order', 'nb_commandes']].copy()\n",
    "\n",
    "quick_look(rfm, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_look(rfm.drop('customer_unique_id', axis=1), True)\n",
    "# OK, c'est ici que le doublon apparait.\n",
    "\n",
    "# Ca veut dire que parmis nos 95 000 clients, il y en a 2, différents, qui ont validé leurs commandes,\n",
    "# du mm montant, à la mm seconde. Etonnant non ?\n",
    "\n",
    "# fun fact\n",
    "# The probability that two out of 100,000 randomly selected dates (with second precision) within the\n",
    "# 63,072,000 seconds in two years will be equal is extremely low. In fact, it's highly unlikely to occur.\n",
    "\n",
    "# Well it just did anyway.\n",
    "# Does make u think. How stuff that are so highly unlikely to occur happen like all the time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pb type\n",
    "\n",
    "rfm['most_recent_order'] = pd.to_datetime(rfm['most_recent_order'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "rfm['most_recent_order'].hist(bins=30, edgecolor='k')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of most_recent_order')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering en vue du clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy model, baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Je me disais\n",
    "# que pour commencer\n",
    "# on pourait déjà étudier un clustering simple,\n",
    "# voire simpliste,\n",
    "# un cluster unique.\n",
    "\n",
    "# Autrement dit, commencer par \"jeter un coup d'oeil\" au \"client moyen\" (/médian /modal?)\n",
    "# Il y a des cas ou la moyenne ne signifie pas forcément grand chose,\n",
    "# ou ne correspond à aucune réalité. Ici cependant, je parie qu'on peut littéralement le trouver.\n",
    "# Ou presque. Challenge trouver Malcolm !\n",
    "\n",
    "# + sérieusement,\n",
    "# Ca a beau être une construction statistique, ça donne une première idée.\n",
    "# Du point de vue entreprise, c'est aussi une mesure simple qui permet de faire des estimations\n",
    "# (ex : un restaurant qui commande à ses fournisseurs)\n",
    "\n",
    "# Enfin, sur le projet précédent (modèles prédictifs), on avait commencé par établir une base line\n",
    "# avec un dummy model. Pour mettre en place le code simplement, et surtout avoir un point de comparaison,\n",
    "# un ordre de grandeur.\n",
    "# Ici je me dis que ça pourrait être utile aussi de faire comme ça du coup,\n",
    "# notamment pour la troisième partie (stabilité du modèle dans le temps, maintenance)\n",
    "\n",
    "# Note du futur : no comment...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculons Malcolm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malcolm = rfm.head(1).copy()\n",
    "\n",
    "malcolm['customer_unique_id'] = 'Malcolm'\n",
    "malcolm['payment_total'] = rfm['payment_total'].mean()\n",
    "malcolm['nb_commandes'] = rfm['nb_commandes'].mean()\n",
    "malcolm['most_recent_order'] = rfm['most_recent_order'].mean()\n",
    "\n",
    "display(malcolm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ca fait presque 8 mois que Malcolm n'a rien acheté.\n",
    "# 1.034008 ça c un bon exemple de cas où le mode a plus de sens que la moyenne.\n",
    "malcolm['nb_commandes'] = rfm['nb_commandes'].mode()\n",
    "# 1\n",
    "\n",
    "# Malcolm existe-t-il ?\n",
    "# La date de la commande est enregistrée avec une précision d'une seconde, donc très peu probable.\n",
    "# Mais on peut trouver qui est le plus proche de lui.\n",
    "# Cherchons parmi les clients qui n'ont fait qu'une commande (les 97%)\n",
    "\n",
    "subset_1_order = rfm.loc[rfm['nb_commandes'] == 1, :].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### graph classique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(subset_1_order['most_recent_order'], subset_1_order['payment_total'], s=30, c='blue', label='Data Points')\n",
    "plt.xlabel('Most Recent Order Date')\n",
    "plt.ylabel('Payment Total')\n",
    "plt.title('2D Scatter Plot with no zooming')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add Malcolm, somewhere in the middle\n",
    "plt.scatter(malcolm['most_recent_order'], malcolm['payment_total'], s=100, c='red', label='M')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### graph interactif (zoom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly (interactif)\n",
    "\n",
    "rfm['color'] = \"rgba(0, 0, 255, 0.8)\"\n",
    "malcolm['color'] = \"rgba(255, 0, 0, 0.8)\"\n",
    "\n",
    "# Create a scatter plot with the specified color mapping\n",
    "fig = px.scatter(rfm, x='most_recent_order', y='payment_total', color_discrete_sequence=['blue'])\n",
    "\n",
    "# Customize the plot\n",
    "fig.update_layout(title='Où est Charlie ?')\n",
    "\n",
    "# Create a trace for the additional red point\n",
    "red_point_trace = px.scatter(malcolm, x='most_recent_order', y='payment_total',\n",
    "                             color_discrete_sequence=['red'],\n",
    "                             size=[10],\n",
    "                             hover_name=['Client moyen'])\n",
    "\n",
    "# Add the red point trace to the figure\n",
    "fig.add_trace(red_point_trace.data[0])\n",
    "\n",
    "# Show the modified plot\n",
    "fig.show()\n",
    "\n",
    "# Now just zoom in !\n",
    "\n",
    "# Au passage on voit que la data commence vraiment le 5-6 janvier 2017.\n",
    "# Avant ça on a quelques dizaines (?) de commandes fin octobre 2016, et c quasiment tout :\n",
    "# 2 en septembre, rien en novembre-décembre. Période de tests ?\n",
    "\n",
    "# => On ne pourra pas entrainer nos modèles sur cette période (en partie 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format des dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il faut s'occuper de nos dates,\n",
    "# le format pd.datetime n'étant pas exactement numérique.\n",
    "# Déjà qu'on se prend des alertes juste en visualisant !\n",
    "\n",
    "# Calculate the timestamp for the reference date\n",
    "reference_date = datetime(2018, 8, 31)\n",
    "reference_timestamp = reference_date.timestamp()\n",
    "\n",
    "# Convert the 'most_recent_order' datetime feature into a timestamp\n",
    "rfm['most_recent_order_timestamp'] = rfm['most_recent_order'].apply(lambda x: x.timestamp())\n",
    "\n",
    "# Optionel, pour lire + facilement :\n",
    "rfm['recent_timestamp'] = reference_timestamp - rfm['most_recent_order_timestamp']\n",
    "rfm['recent_timedelta'] = reference_date - rfm['most_recent_order'] # type timedelta\n",
    "rfm['recent_datetime'] = reference_date - rfm['recent_timedelta']\n",
    "\n",
    "quick_look(rfm)\n",
    "\n",
    "# Attention, 'recent_datetime' se lit \"dans l'autre sens\" par rapport aux autres :\n",
    "# = 0 pour la date de référence, augmente qd on remonte vers la passé\n",
    "# max value = commande la + ancienne\n",
    "# (voir histogramme bleu ci-dessous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check everything's ok\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot a histogram for most_recent_order_timestamp on the first subplot\n",
    "axes[0].hist(rfm['most_recent_order'], bins=20, color='skyblue', alpha=0.8)\n",
    "axes[0].set_xlabel('most_recent_order')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of most_recent_order (datetime)')\n",
    "\n",
    "# Plot a histogram for recent on the second subplot\n",
    "axes[1].hist(rfm['most_recent_order_timestamp'], bins=20, color='skyblue', alpha=0.8)\n",
    "axes[1].set_xlabel('most_recent_order_timestamp')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of most_recent_order_timestamp')\n",
    "\n",
    "plt.tight_layout()  # Ensure proper spacing between subplots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot a histogram for most_recent_order_timestamp on the first subplot\n",
    "axes[0].hist(rfm['recent_timestamp'], bins=20, color='lightgreen', alpha=0.8)\n",
    "axes[0].set_xlabel('recent_timestamp')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of recent_timestamp')\n",
    "\n",
    "# Plot a histogram for most_recent_order on the second subplot\n",
    "axes[1].hist(rfm['recent_datetime'], bins=20, color='skyblue', alpha=0.8)\n",
    "axes[1].set_xlabel('recent_datetime')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of recent_datetime')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Nickel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation 3D (toute la data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D scatter plot\n",
    "fig = px.scatter_3d(rfm, x='recent_datetime', y='nb_commandes', z='payment_total')\n",
    "\n",
    "# Customize the plot\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(title='Our data in 3D')\n",
    "\n",
    "fig.update_layout(\n",
    "    width=800,  # Specify the width in pixels\n",
    "    height=600  # Specify the height in pixels\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# On voit à nouveau que notre donnée est très compacte sur les 2 axes correspondant\n",
    "# aux deux features très asymétriques.\n",
    "\n",
    "# Pas sûr que ça pose vraiment problème aux modèles, mais nous on y verra difficilement\n",
    "# si tous les points sont les uns sur les autres.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passage au log (skewed features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va préparer un second jeu de données où les 2 features asym ont été passées au log\n",
    "\n",
    "# R ('recent') n'est pas très asymétrique (skewness = 0.45),\n",
    "# en revanche F et M le sont\n",
    "\n",
    "features_RFM = ['recent_timestamp', 'nb_commandes', 'payment_total'] # = R F M\n",
    "rfm_log = rfm[features_RFM].copy()\n",
    "\n",
    "# valeurs min < 0, mais > -1\n",
    "rfm_log['nb_commandes'] = np.log(rfm['nb_commandes'] + 1)\n",
    "rfm_log['payment_total'] = np.log(rfm['payment_total'] + 1)\n",
    "\n",
    "graphs_analyse_uni(rfm_log, 'nb_commandes', bins=50, r=2, density=True) # F\n",
    "graphs_analyse_uni(rfm_log, 'payment_total', bins=50, r=2, density=True) # M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok c déjà + espacé et symétrique\n",
    "# En tous cas pour le montant\n",
    "# Moins évident pour la fréquence, qui prend des valeurs discrètes\n",
    "\n",
    "fig = px.scatter_3d(rfm_log, x='recent_timestamp', y='nb_commandes', z='payment_total')\n",
    "\n",
    "# Customize the plot\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(title='Our data in 3D')\n",
    "\n",
    "fig.update_layout(\n",
    "    width=800,  # Specify the width in pixels\n",
    "    height=600  # Specify the height in pixels\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# + aéré\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut scale\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the selected columns\n",
    "rfm_scaled = rfm[features_RFM].copy()\n",
    "rfm_scaled[features_RFM] = scaler.fit_transform(rfm[features_RFM])\n",
    "\n",
    "quick_look(rfm_scaled)\n",
    "rfm_scaled.describe()\n",
    "\n",
    "# Do the same for log dataset\n",
    "rfm_log_scaled = rfm_log[features_RFM].copy()\n",
    "rfm_log_scaled[features_RFM] = scaler.fit_transform(rfm_log[features_RFM])\n",
    "\n",
    "quick_look(rfm_log_scaled)\n",
    "rfm_log_scaled.describe()\n",
    "\n",
    "# Juste le tps de regarder à quoi ressemblent nos nouvelles distribs,\n",
    "# et on est prêt pour notre premier k-means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observons nos nouvelles distribs\n",
    "\n",
    "graphs_analyse_uni(rfm_scaled, 'recent_timestamp', bins=50, r=2, density=True)\n",
    "# graphs_analyse_uni(rfm_log_scaled, 'recent', bins=50, r=2, density=True)\n",
    "# (c la mm chose, on n'a pas log cette feature)\n",
    "\n",
    "# Lorenz ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_analyse_uni(rfm_scaled, 'nb_commandes', bins=50, r=2, density=True) # F\n",
    "graphs_analyse_uni(rfm_log_scaled, 'nb_commandes', bins=50, r=2, density=True) # F\n",
    "\n",
    "# Gini WTF ??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_analyse_uni(rfm_scaled, 'payment_total', bins=50, r=2, density=True) # M\n",
    "graphs_analyse_uni(rfm_log_scaled, 'payment_total', bins=50, r=2, density=True) # M\n",
    "\n",
    "# J'avais jamais vu des courbes de Lorenz comme ça\n",
    "# Je devrais peut-être aller dormir ^^\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitionnement manuel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les moyennes sont bien à zero, et les variances à 1.\n",
    "\n",
    "# Le problème de distributions très asymétriques semble corrigé par le passage au log.\n",
    "# A vérifier pour nb_commandes > 1\n",
    "\n",
    "# Il est possissible qu'on soit amené à encore étudier séparément\n",
    "# les 97% de clients qui n'ont fait qu'une commande (en 2D du coup),\n",
    "# et les 3% de clients \"réguliers\" (nb_commandes >= 2) (en 3D).\n",
    "\n",
    "# On a déjà créé un subset pour les 97%, mais on a modifié les dates depuis.\n",
    "subset_1_order  = rfm.loc[rfm['nb_commandes'] == 1, features_RFM].copy()\n",
    "\n",
    "# Pour les 3% :\n",
    "subset_several_orders = rfm.loc[rfm['nb_commandes'] > 1, features_RFM].copy()\n",
    "\n",
    "# log\n",
    "subset_1_order_log = subset_1_order[features_RFM].copy()\n",
    "subset_1_order_log['nb_commandes'] = np.log(subset_1_order['nb_commandes'] + 1)\n",
    "subset_1_order_log['payment_total'] = np.log(subset_1_order['payment_total'] + 1)\n",
    "\n",
    "subset_several_orders_log = subset_several_orders.copy() # On a déjà sélectionné les features\n",
    "subset_several_orders_log['nb_commandes'] = np.log(subset_several_orders['nb_commandes'] + 1)\n",
    "subset_several_orders_log['payment_total'] = np.log(subset_several_orders['payment_total'] + 1)\n",
    "\n",
    "# scaling\n",
    "subset_1_order_scaled = subset_1_order[features_RFM].copy()\n",
    "subset_1_order_scaled[features_RFM] = scaler.fit_transform(subset_1_order[features_RFM])\n",
    "\n",
    "subset_several_orders_scaled = subset_several_orders.copy()\n",
    "subset_several_orders_scaled[features_RFM] = scaler.fit_transform(subset_several_orders[features_RFM])\n",
    "\n",
    "# log + scaling\n",
    "subset_1_order_log_scaled = subset_1_order_log[features_RFM].copy()\n",
    "subset_1_order_log_scaled[features_RFM] = scaler.fit_transform(subset_1_order_log[features_RFM])\n",
    "\n",
    "subset_several_orders_log_scaled = subset_several_orders_log[features_RFM].copy()\n",
    "subset_several_orders_log_scaled[features_RFM] = scaler.fit_transform(subset_several_orders_log[features_RFM])\n",
    "\n",
    "# Encore une fausse bonne idée (Captain Hinsight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 k-means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### les scores que nous allons utiliser pour évaluer la qualité de nos clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On commence 2 par 2 en 2D,\n",
    "# où on essaie direct les 3 ensemble ?\n",
    "\n",
    "# On va tenter la 3D pour avoir directement une vue d'ensemble.\n",
    "\n",
    "# Un petit mot sur nos \"scores\" ?\n",
    "\n",
    "# L'inertie (moyenne des carrés des distances entre points et centroide)\n",
    "# est le plus rapide à calculer.\n",
    "# Encore que en pratique, c serré avec tightness. et avec Davies-Bouldin.Bbref.\n",
    "# L'inertie nous donne une idée sur la dispersion des points dans le cluster, par rapport au centroide,\n",
    "# sur la variance intracluster\n",
    "# Méthode du coude\n",
    "\n",
    "# Assez similaire, le coeff de Tightness, un peu plus long à calculer, nous dit en + si notre cluster\n",
    "# est compact ou pas, si tous les points sont plutôt resserrés ou distants les uns des autres, en moyenne.\n",
    "# + petit = mieux pour nous, bon cluster !\n",
    "\n",
    "# En + de 2 ou 3 dimensions, je me dis que l'inertie commence à avoir plus de sens car\n",
    "# les distances vont exploser, mais la variance pas forcément.\n",
    "# Du coup un score + stable ?\n",
    "\n",
    "# Tightness est aussi le seul score sans raccourci ds sckikit.\n",
    "# G l'impression qu'inertia est + utilisé.\n",
    "\n",
    "# La Separation nous dit si nos clusters sont (bien) éloignés.\n",
    "# Ca aussi on préfère :)\n",
    "# Combinée aux Tightness des différents clusters, elle forme le score de Davies_Bouldin,\n",
    "# DB = (T moyen) / (S moyen)\n",
    "\n",
    "# Enfin l'indicateur le plus utilisé,\n",
    "# et malheureusement, le + long à calculer bien sûr :)\n",
    "# Le silhouette score, qui vérifie si chaque point est placé dans le \"bon\" cluster\n",
    "# = difference entre (moyenne distances entre point et autres points de son cluster)\n",
    "# et (idem cluster voisin (le plus proche))\n",
    "# normalisée par le + gd des 2 -> résultat compris dans [-1, 1]\n",
    "# ca c pour 1 point, le silhouette score en pratique c la moyenne de ca pour ts les points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombre optimal de clusters, using different scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inertia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the score values for different numbers of clusters\n",
    "def plot_score(score_list, score_name, max=15):\n",
    "    \"\"\"\n",
    "    Plot the score values for different numbers of clusters.\n",
    "\n",
    "    Args:\n",
    "        score_list (list): A list of score values for different numbers of clusters.\n",
    "        score_name (str): The name of the score being plotted (e.g., 'Silhouette Score').\n",
    "        max (int, optional): The maximum number of clusters to plot (default is 15).\n",
    "\n",
    "    This function generates a line plot to visualize how a specific clustering score varies with different numbers of clusters (k).\n",
    "    It helps in identifying the optimal number of clusters using the given clustering score.\n",
    "\n",
    "    Parameters:\n",
    "    - score_list: A list of score values for different values of k.\n",
    "    - score_name: The name of the score to display in the plot (e.g., 'Silhouette Score', 'Inertia', etc.).\n",
    "    - max: The maximum number of clusters to plot (default is 15).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(2, max), score_list, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel(score_name)\n",
    "    plt.title('Optimal k, using ' + score_name + ' score')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Calculate Tightness\n",
    "def compute_tightness(data, cluster_labels, cluster_centers):\n",
    "    \"\"\"\n",
    "    Calculate the tightness score for a clustering.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): The data points.\n",
    "        cluster_labels (numpy.ndarray): Labels indicating the cluster assignment for each data point.\n",
    "        cluster_centers (numpy.ndarray): The centers of the clusters.\n",
    "\n",
    "    Returns:\n",
    "        float: The tightness score, a measure of how close data points are to their cluster centers.\n",
    "\n",
    "    The tightness score quantifies the compactness of clusters in a clustering. It is computed as the average distance of data points within each cluster to their respective cluster center. A lower tightness score indicates that data points are closer to their cluster centers, indicating tighter clusters.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The data points in the dataset.\n",
    "    - cluster_labels: An array of cluster labels for each data point.\n",
    "    - cluster_centers: The centers of the clusters.\n",
    "    \"\"\"\n",
    "    tightness_score = 0\n",
    "    for i in range(len(cluster_centers)):\n",
    "        cluster_points = data[cluster_labels == i]\n",
    "        center = cluster_centers[i]\n",
    "        distances = np.linalg.norm(cluster_points - center, axis=1)\n",
    "        tightness_score += np.mean(distances)\n",
    "    tightness_score /= len(cluster_centers)\n",
    "    return tightness_score\n",
    "\n",
    "\n",
    "def find_best_nb_clusters_for_kmeans(df=rfm_scaled, features=features_RFM, scores=[], max=15):\n",
    "    \"\"\"\n",
    "    Find the best number of clusters for K-Means clustering based on various scoring methods.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset for clustering.\n",
    "        features (list): List of feature columns for clustering.\n",
    "        scores (list): List of scoring methods to use ('inertia', 'tightness', 'db', 'silhouette').\n",
    "        max (int): The maximum number of clusters to consider (default is 15).\n",
    "\n",
    "    This function performs K-Means clustering with different numbers of clusters (k) and evaluates the clustering results using various scoring methods, including inertia, tightness, Davies-Bouldin index, and silhouette score. It then plots the scores to help determine the optimal number of clusters based on the chosen scoring methods.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The dataset for clustering.\n",
    "    - features: List of feature columns for clustering.\n",
    "    - scores: List of scoring methods to use ('inertia', 'tightness', 'db', 'silhouette').\n",
    "    - max: The maximum number of clusters to consider (default is 15).\n",
    "    \"\"\"\n",
    "    # Create empty lists to store scores for different k values\n",
    "    inertia_scores = []\n",
    "    tightness_scores = []\n",
    "    davies_bouldin_scores = []\n",
    "    silhouette_scores = []\n",
    "\n",
    "    # Test different numbers of clusters from 2 to 15\n",
    "    for k in range(2, max):\n",
    "        kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "        kmeans.fit(df[features])\n",
    "\n",
    "        if 'inertia' in scores:\n",
    "            inertia_scores.append(kmeans.inertia_)\n",
    "        if 'tightness' in scores:\n",
    "            tightness = compute_tightness(df[features], kmeans.labels_, kmeans.cluster_centers_)\n",
    "            tightness_scores.append(tightness)\n",
    "        if 'db' in scores:\n",
    "            davies_bouldin = davies_bouldin_score(df[features], kmeans.labels_)\n",
    "            davies_bouldin_scores.append(davies_bouldin)\n",
    "        if 'silhouette' in scores:\n",
    "            silhouette = silhouette_score(df[features], kmeans.labels_)\n",
    "            silhouette_scores.append(silhouette)\n",
    "\n",
    "    if 'inertia' in scores:\n",
    "        plot_score(inertia_scores, 'inertia', max=max)\n",
    "    if 'tightness' in scores:\n",
    "        plot_score(tightness_scores, 'tightness', max=max)\n",
    "    if 'db' in scores:\n",
    "        plot_score(davies_bouldin_scores, 'Davies-Bouldin', max=max)\n",
    "    if 'silhouette' in scores:\n",
    "        plot_score(silhouette_scores, 'silhouette', max=max)\n",
    "\n",
    "\n",
    "find_best_nb_clusters_for_kmeans(scores=['inertia'], max=15)\n",
    "\n",
    "# best nb de cluster : 4 (elbow method)\n",
    "# tps : 5-6s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tightness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_nb_clusters_for_kmeans(scores=['tightness'], max=15)\n",
    "\n",
    "# best nb de cluster : 5 (min)\n",
    "# tps : 5-6s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Davies-Bouldin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_nb_clusters_for_kmeans(df=rfm_scaled, scores=['db'], max=15)\n",
    "\n",
    "# best nb de cluster : 4 (min)\n",
    "# tps : 5-6s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset, sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les temps de calcul vont augmenter très vite pour silhouette. On va avoir besoin de subsets\n",
    "# (Calculer seulement l'inertie prend 5 seconds. Avec les 3 scores, il faut une heure...)\n",
    "\n",
    "# Les résultats sont concordants (ici, 4 clusters, 5 si on privilégie tightness)\n",
    "\n",
    "# utiliser seulement l'inertie ?\n",
    "# -> Subsampling ?\n",
    "# Parallel Processing ?\n",
    "\n",
    "rfm_scaled_tenth = rfm_scaled[::10].copy()\n",
    "\n",
    "# Vérifions que la forme du subset est la même\n",
    "# on a selectionné 1 ligne ttes les 10 lignes, donc c une selection aléatoire,\n",
    "# tant que les données de départ ne sont pas ordonnées (modulo 10).\n",
    "\n",
    "fig = px.scatter_3d(rfm_scaled, x='recent_timestamp', y='nb_commandes', z='payment_total')\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(title='Our data in 3D')\n",
    "fig.update_layout(\n",
    "    width=600,  # Specify the width in pixels\n",
    "    height=400  # Specify the height in pixels\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter_3d(rfm_scaled_tenth, x='recent_timestamp', y='nb_commandes', z='payment_total')\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(title='Our data in 3D')\n",
    "fig.update_layout(\n",
    "    width=600,  # Specify the width in pixels\n",
    "    height=400  # Specify the height in pixels\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Ajuster ?\n",
    "# Ca a l'air pas mal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = go.Figure()\n",
    "\n",
    "scatter1 = go.Scatter3d(x=rfm_scaled['recent_timestamp'], y=rfm_scaled['nb_commandes'], z=rfm_scaled['payment_total'], mode='markers', marker=dict(size=5))\n",
    "fig1.add_trace(scatter1)\n",
    "fig1.update_layout(title='Our data in 3D - Subset 1')\n",
    "fig1.update_layout(width=400, height=300)\n",
    "\n",
    "# Create the second scatter plot\n",
    "fig2 = go.Figure()\n",
    "\n",
    "scatter2 = go.Scatter3d(x=rfm_scaled_tenth['recent_timestamp'], y=rfm_scaled_tenth['nb_commandes'], z=rfm_scaled_tenth['payment_total'], mode='markers', marker=dict(size=5))\n",
    "fig2.add_trace(scatter2)\n",
    "fig2.update_layout(title='Our data in 3D - Subset 2')\n",
    "fig2.update_layout(width=400, height=300)\n",
    "\n",
    "# Combine the two figures\n",
    "fig = go.Figure(data=[fig1.data[0], fig2.data[0]])\n",
    "fig.show()\n",
    "\n",
    "# Je crois qu'on voyait mieux séparés\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va donc utiliser ce subset pour aller + vite\n",
    "\n",
    "# find_best_nb_clusters_for_kmeans(df=rfm_scaled_tenth, scores=['silhouette'], max=15)\n",
    "\n",
    "# best nb de cluster : 4 (max)\n",
    "# tps : environ 1 min, sur 1/10 des datas\n",
    "\n",
    "# Mm resultat sur dataset complet, mais bcp + long, environ 45 min\n",
    "# find_best_nb_clusters_for_kmeans(df=rfm_scaled, scores=['silhouette'], max=15)\n",
    "Image(filename='img/silhouette_rfm.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yellowbrick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans(n_init='auto')\n",
    "visualizer = KElbowVisualizer(model, k=(2,12))\n",
    "\n",
    "visualizer.fit(rfm_scaled)    # Fit the data to the visualizer\n",
    "visualizer.poof()    # Draw/show/poof the data\n",
    "\n",
    "# bcp, bcp + rapide que notre méthode (silhouette_score, from sklearn.metrics)\n",
    "# Comment ils font ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans(n_init='auto')\n",
    "visualizer = KElbowVisualizer(model, k=(2,12))\n",
    "\n",
    "visualizer.fit(rfm_scaled)    # Fit the data to the visualizer\n",
    "visualizer.poof()    # Draw/show/poof the data\n",
    "\n",
    "# Comment c si rapide ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans(n_init='auto')\n",
    "visualizer = KElbowVisualizer(model, k=(2,12), metric='calinski_harabasz', timings=False)\n",
    "\n",
    "visualizer.fit(rfm_scaled)    # Fit the data to the visualizer\n",
    "visualizer.poof()    # Draw/show/poof the data\n",
    "\n",
    "# 3-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(4, n_init='auto')\n",
    "visualizer = SilhouetteVisualizer(model)\n",
    "\n",
    "# visualizer.fit(rfm_scaled)    # Fit the data to the visualizer\n",
    "# visualizer.poof()    # Draw/show/poof the data\n",
    "\n",
    "# + long\n",
    "# 2 gros clusters, 2 bcp - étendus\n",
    "\n",
    "Image(filename='img/poof.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stabilité\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etude de la stabilité des clusters sur une sous-partition aléatoire\n",
    "# methodo pas bon là\n",
    "# Pourquoi j'ai codé ça ??\n",
    "\n",
    "# Il faut verif si j'ai les mm clusters, pas si le nb reste le mm sur une ss partit aleatoire !\n",
    "\n",
    "# Remarque, c tjs une bonne nouvelle que le clustering semble stable sur de \"petites\" partitions\n",
    "# au 1/20 ième : cela correspond environ à 1 mois de data pour la partie 3.\n",
    "\n",
    "# Par contre, encore une fois, pas certain que ce soit encore le cas en 4 ou 5 dimensions.\n",
    "# A chaque dimension ajoutée, toute la data est étirée le long d'un nouvel axe,\n",
    "# ce qui crée beaucoup de vide.\n",
    "# Du coup il faudra peut-être bcp + de points pour qu'un subset construit par sample ait des chances de\n",
    "# vaguement reproduire la forme globale. A tester\n",
    "\n",
    "n_iter = 10\n",
    "\n",
    "for i in range(n_iter):\n",
    "    # Shuffle the DataFrame randomly\n",
    "    shuffled_df = rfm_scaled.sample(frac=1, random_state=i)  # frac=1 shuffles all rows\n",
    "\n",
    "    # Sample a portion of the shuffled DataFrame\n",
    "    sample_size = 5000  # Adjust this to your desired sample size\n",
    "    sampled_df = shuffled_df.head(sample_size)\n",
    "\n",
    "    inertia = []\n",
    "\n",
    "    for k in range(2, 11):\n",
    "        kmeans = KMeans(n_clusters=k, n_init=10, random_state=i)\n",
    "        kmeans.fit(shuffled_df[features_RFM])\n",
    "\n",
    "        inertia.append(kmeans.inertia_)\n",
    "\n",
    "    print(inertia)\n",
    "\n",
    "    # Plot the inertia values for different numbers of clusters\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(2, 11), inertia, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method for Optimal k')\n",
    "    plt.grid(True)\n",
    "\n",
    "# Très stable\n",
    "# Vérifier mieux que les clusters sont proches, similaires : -> utiliser l'ARI ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous allons avoir besoin d'une fonction qui retourne les labels assignés par le k-means\n",
    "# (pour visualisation et ARI).\n",
    "\n",
    "# Assuming 4 is the optimal number of clusters\n",
    "def assign_labels_using_kmeans(alea, df=rfm_scaled, features=features_RFM, best_k=4, silhouette=False):\n",
    "    \"\"\"\n",
    "    Assign cluster labels to data points using K-Means clustering.\n",
    "\n",
    "    Args:\n",
    "        alea (int): Random seed for reproducibility.\n",
    "        df (pd.DataFrame): The dataset for clustering.\n",
    "        features (list): List of feature columns for clustering.\n",
    "        best_k (int): The number of clusters for K-Means (default is 4).\n",
    "        silhouette (bool): Whether to print the silhouette score for the K-Means clustering (default is False).\n",
    "\n",
    "    This function performs K-Means clustering on the given data with a specified number of clusters (best_k) and assigns cluster labels to data points. Optionally, it can print the silhouette score as a measure of clustering quality.\n",
    "\n",
    "    Returns:\n",
    "    - labels (array): Cluster labels assigned to data points.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=best_k, n_init='auto', random_state=alea)\n",
    "    kmeans.fit(df[features])\n",
    "\n",
    "    if silhouette == True:\n",
    "        print('silhouette_score for kmeans = ', silhouette_score(df[features], kmeans.labels_))\n",
    "\n",
    "    return kmeans.labels_\n",
    "\n",
    "# L'ARI est souvent utilisé pour comparer le clustering obtenu par un modèle à une partition connue,\n",
    "# pour tester la qualité du modèle. Ici c'est un peu différent,\n",
    "# Nous n'avons pas de partition pré-établie. Nous allons seulement tester la stabilité,\n",
    "# pas la qualité, en comparant les labels obtenus successivement à ceux de l'essai précédent.\n",
    "\n",
    "labels_kmeans = []\n",
    "\n",
    "for i in range(0, 15):\n",
    "    labels = assign_labels_using_kmeans(alea=i)\n",
    "    labels_kmeans.append(labels)\n",
    "    if i > 0:\n",
    "        # Calculate ARI\n",
    "        ari = adjusted_rand_score(labels_kmeans[i], labels_kmeans[i-1])\n",
    "        print(f'ARI = {ari}', '\\n')\n",
    "\n",
    "display(labels_kmeans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation, premier clustering RFM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wrong legend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ca a l'air super stable.\n",
    "# Visualisons !\n",
    "\n",
    "def plot_kmeans_3D(df=rfm_scaled, legend=False):\n",
    "    # Create a list of colors based on labels\n",
    "    cluster_colors = px.colors.qualitative.Set2[:len(set(labels_kmeans[0]))]\n",
    "\n",
    "    fig = px.scatter_3d(df, x='recent_timestamp', y='nb_commandes', z='payment_total', color=labels_kmeans[0],\n",
    "                        color_discrete_sequence=cluster_colors, opacity=0.8)\n",
    "\n",
    "    fig.update_traces(marker=dict(size=5), showlegend=False)\n",
    "    fig.update_layout(title='K-Means Clustering in 3D')\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=800,  # Specify the width in pixels\n",
    "        height=600  # Specify the height in pixels\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_kmeans_3D()\n",
    "\n",
    "# la légende est displayed qd mm ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### legend ok for now, wrong colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La légende sert pas à gd chose ici, c + clair sans.\n",
    "# Ou en mode discret (catégorique)\n",
    "\n",
    "def plot_kmeans_3D(df=rfm_scaled, legend=True):\n",
    "    # Convert cluster labels to integer and then to string format\n",
    "    cluster_labels_str = labels_kmeans[0].astype(int).astype(str)\n",
    "    # Create a list of colors based on labels\n",
    "    cluster_colors = px.colors.qualitative.Set1[:len(set(labels_kmeans[0]))]\n",
    "\n",
    "    # Create a 3D scatter plot with custom legend\n",
    "    fig = px.scatter_3d(df, x='recent_timestamp', y='nb_commandes', z='payment_total', color=cluster_labels_str,\n",
    "                        color_discrete_sequence=cluster_colors, opacity=0.7)\n",
    "\n",
    "    fig.update_traces(marker=dict(size=5), showlegend=legend)\n",
    "    fig.update_layout(title='K-Means Clustering in 3D')\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=800,  # Specify the width in pixels\n",
    "        height=600  # Specify the height in pixels\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_kmeans_3D(legend=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### more color tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests couleurs\n",
    "\n",
    "custom_color_set_4 = ['red', 'pink', 'lightgreen', 'yellow']\n",
    "\n",
    "# Define a function to plot K-Means clusters in 3D with a custom color set\n",
    "def plot_kmeans_3D(df=rfm_scaled, legend=True, color_set=custom_color_set_4, opacity=1.0):\n",
    "    \"\"\"\n",
    "    Visualize K-Means clustering in 3D using Plotly Express.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing the data.\n",
    "        legend (bool): Whether to display the legend (default is False).\n",
    "\n",
    "    This function creates a 3D scatter plot of K-Means clustering results using Plotly Express. It visualizes the data points based on their coordinates in three dimensions (recent_timestamp, nb_commandes, payment_total) and colors them according to the cluster labels.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame.\n",
    "    - legend (bool): Set to True to display the legend.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Convert cluster labels to integer and then to string format\n",
    "    cluster_labels_str = labels_kmeans[0].astype(int).astype(str)\n",
    "    # Create a list of colors based on labels\n",
    "    if color_set is None:\n",
    "        cluster_colors = custom_color_set_4[:len(set(labels_kmeans[0]))]\n",
    "    else:\n",
    "        cluster_colors = color_set[:len(set(labels_kmeans[0]))]\n",
    "\n",
    "    # Create a 3D scatter plot with custom legend\n",
    "    fig = px.scatter_3d(df, x='recent_timestamp', y='nb_commandes', z='payment_total', color=cluster_labels_str,\n",
    "                        color_discrete_sequence=cluster_colors, opacity=opacity)\n",
    "\n",
    "    fig.update_traces(marker=dict(size=5), showlegend=legend)\n",
    "    fig.update_layout(title='K-Means Clustering in 3D')\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=800,  # Specify the width in pixels\n",
    "        height=600  # Specify the height in pixels\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Example usage with a different color set\n",
    "plot_kmeans_3D(color_set=px.colors.qualitative.Set3)\n",
    "# plot_kmeans_3D(color_set=px.colors.qualitative.Prism)\n",
    "\n",
    "list_other_color_sets = [px.colors.qualitative.Set3,\n",
    "                         px.colors.qualitative.Dark24,\n",
    "                         px.colors.qualitative.Pastel1,\n",
    "                         px.colors.qualitative.Pastel2,\n",
    "                         px.colors.qualitative.Prism,\n",
    "                         px.colors.qualitative.Vivid,\n",
    "                         px.colors.qualitative.Safe]\n",
    "\n",
    "for color_set in list_other_color_sets:\n",
    "    print(f\"Using color set: {color_set}\")\n",
    "    # plot_kmeans_3D(color_set=color_set)\n",
    "\n",
    "# ! OK, Pas abuser de plotly ds la mm cell !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans_3D(color_set=px.colors.qualitative.Pastel1)\n",
    "# plot_kmeans_3D(color_set=px.colors.qualitative.Pastel2)\n",
    "\n",
    "# J'aime bcp les pastels, mais c pas le + lisible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_kmeans_3D(color_set=px.colors.qualitative.Vivid)\n",
    "# plot_kmeans_3D(color_set=px.colors.qualitative.Safe)\n",
    "\n",
    "# Décommentez à vos risques et périls. C très moche^^\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom colormap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Au final, jamais mieux servi...\n",
    "plot_kmeans_3D()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultat, interprétation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut déjà interpréter les clusters obtenus, cependant nos données de base sont très compactes,\n",
    "# on verra peut-être mieux en utilisant le jeu transformé.\n",
    "\n",
    "plot_kmeans_3D(df=rfm_log_scaled)\n",
    "\n",
    "# On voit mieux, merci la fonction log !\n",
    "\n",
    "# En jaune (cluster 3), les clients qui ont le plus dépensé au total\n",
    "# en vert (cluster 2) les clients réguliers\n",
    "# en rouge (cluster 0) les nveaux clients (1 petit achat récent)\n",
    "# en rose (cluster 1) les pires clients : 1 petit achat il y a lgtmps.\n",
    "\n",
    "# C'est un bon début : le modèle est stable, les clusters ont du sens du point de vue métier.\n",
    "# Cependant Olis attend une analyse + précise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 dendro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sampling obligatoire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on a small sample (fastest but very distorted data)\n",
    "# dendro_df = rfm_log_scaled[features_RFM][::50].copy()\n",
    "\n",
    "# Use complete data : impossible without modifying notebook params\n",
    "# (dataset too big for an algo with this order of complexity)\n",
    "# dendro_df = rfm_log_scaled[features_RFM].copy()\n",
    "\n",
    "# Good compromise, on a déjà vérifié que la topologie des datas est conservée assez fidèlement\n",
    "dendro_df = rfm_log_scaled[::10].copy()\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(dendro_df, method='ward')  # You can choose different linkage methods\n",
    "\n",
    "# Create a dendrogram\n",
    "dendrogram(linkage_matrix, labels=dendro_df.index.values)\n",
    "\n",
    "# Display the dendrogram\n",
    "plt.title(\"Dendrogram\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "plt.axhline(y=82, color='orange', linestyle='--', label='Threshold')\n",
    "# best number of clusters = 4 with this first \"method\"\n",
    "# (More like a quick empirical \"rule of thumb\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Bcp + long que k-means !\n",
    "\n",
    "# Sur [::10]\n",
    "display(Image(filename='img/dendro_rfm.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nb optimal de cluster, using inconsistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters using inconsistency\n",
    "depth = 5  # You can adjust this depth parameter\n",
    "inconsistencies = inconsistent(linkage_matrix, depth)\n",
    "optimal_cluster_count = inconsistencies.argmax() + 1\n",
    "\n",
    "# Plot the inconsistency values\n",
    "plt.figure()\n",
    "plt.plot(range(2, len(inconsistencies) + 2), inconsistencies)\n",
    "plt.title(\"Inconsistency vs. Number of Clusters\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Inconsistency\")\n",
    "plt.axvline(x=optimal_cluster_count, color='red', linestyle='--', label='Optimal Cluster Count')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 38 000 clusters, c peut-être mathématiquement optimal, mais...\n",
    "# Bon courage pour l'interprétation !\n",
    "\n",
    "# Plot the inconsistency values for up to 15 clusters\n",
    "max_clusters_to_plot = 15\n",
    "optimal_cluster_count = inconsistencies[:max_clusters_to_plot - 1].argmax() + 2 # index starts at 2\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(range(2, max_clusters_to_plot + 2), inconsistencies[:max_clusters_to_plot])\n",
    "\n",
    "plt.title(\"Inconsistency vs. Number of Clusters\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Inconsistency\")\n",
    "plt.axvline(x=optimal_cluster_count, color='red', linestyle='--', label='Optimal Cluster Count')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# On retrouve nos 4 clusters\n",
    "# or do we ?\n",
    "\n",
    "optimal_cluster_count = 4\n",
    "\n",
    "# Extract cluster labels\n",
    "cluster_labels_dendro_4 = fcluster(linkage_matrix, t=optimal_cluster_count, criterion='maxclust')\n",
    "\n",
    "# Compare dendro clusters to kmeans clusters\n",
    "labels_kmeans4_tenth = assign_labels_using_kmeans(alea=42, df=dendro_df, features=features_RFM, best_k=4, silhouette=True)\n",
    "ari = adjusted_rand_score(labels_kmeans4_tenth, cluster_labels_dendro_4)\n",
    "print(f'ARI = {ari}', '\\n')\n",
    "\n",
    "# J'avais oublié qu'on travaillait d'abord sur un petit subset pour gagner du tps...\n",
    "# Décommenter l'ARI ci-dessus seulement sur jeu sliced au 1/10 !\n",
    "# C bon, c corrigé,\n",
    "\n",
    "# Silhouette kmeans = 0.37\n",
    "# ARI = 0.6, les 2 partitionnements semblent assez différents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualité des clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enfin, évaluons la qualité des clusters obtenus\n",
    "\n",
    "def evaluate_dendro_clusters(linkage='ward'):\n",
    "    \"\"\"\n",
    "    Evaluate the optimal number of clusters using Agglomerative Clustering and Silhouette Score.\n",
    "\n",
    "    Args:\n",
    "        linkage (str): The linkage method for Agglomerative Clustering (default is 'ward').\n",
    "\n",
    "    This function performs Agglomerative Clustering with varying numbers of clusters and calculates the Silhouette Score for each clustering. It then plots the Silhouette Score against the number of clusters and identifies the optimal number of clusters based on the highest Silhouette Score.\n",
    "\n",
    "    Parameters:\n",
    "    - linkage (str): The linkage method for Agglomerative Clustering.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for n_clusters in range(2, 10):\n",
    "        # Create an AgglomerativeClustering model with the chosen linkage method and the current number of clusters\n",
    "        model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
    "\n",
    "        # Fit the model to your data\n",
    "        cluster_labels = model.fit_predict(dendro_df)\n",
    "\n",
    "        # Calculate the silhouette score for the current clustering\n",
    "        silhouette_avg = silhouette_score(dendro_df, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Plot the silhouette scores\n",
    "    plt.figure()\n",
    "    plt.plot(range(2, 10), silhouette_scores, marker='o', linestyle='-', color='b')\n",
    "    plt.title(\"Silhouette Score vs. Number of Clusters\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Find the optimal number of clusters (highest silhouette score)\n",
    "    optimal_clusters = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "    plt.axvline(x=optimal_clusters, color='r', linestyle='--', label=f'Optimal Clusters ({optimal_clusters})')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "evaluate_dendro_clusters()\n",
    "\n",
    "# Pour nb clusters > 2, on retrouve encore 4 comme meilleure valeur.\n",
    "# Le silhouette score du kmeans était meilleur.\n",
    "# Tant mieux, l'algo le + rapide est aussi le + performant ici.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testons d'autres méthodes\n",
    "\n",
    "evaluate_dendro_clusters(linkage='single')\n",
    "\n",
    "# ??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dendro_clusters(linkage='complete')\n",
    "\n",
    "# le silhouette score est curieux ici aussi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dendro_clusters(linkage='average')\n",
    "\n",
    "# c quoi ces silhouettes ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour l'instant le kmeans l'emporte,\n",
    "\n",
    "# - pour la qualité supérieure de son clustering, comme le montre le silhouette score\n",
    "# (particulièrement significatif, je trouve, qd on compare le clustering du kmeans\n",
    "# au clustering hiérarchique utilisant la méthode de ward. En effet, tous deux cherchent à minimiser\n",
    "# la variance intracluster.)\n",
    "\n",
    "# - pour sa capacité à travailler sur le jeu de données entier, rapidement.\n",
    "# En effet la \"limitation\" du clustering hiérarchique (sa complexité algorithmique lourde)\n",
    "# sera encore plus accentuée à chaque ajout de feature.\n",
    "\n",
    "# Notons cependant que pour l'étape finale, le modèle choisi ne travaillera plus que sur une tranche\n",
    "# réduite du dataset, mise à jour régulièrement. Le clustering hiérarchique reste donc une possibilité\n",
    "# pour l'instant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"à la main\"\n",
    "\n",
    "def plot_DBSCAN_3D(df=rfm_log_scaled, legend=True, color_set=custom_color_set_4, opacity=1.0):\n",
    "    # Specify the DBSCAN parameters (eps and min_samples)\n",
    "    eps = 0.2  # Maximum distance to form a dense cluster\n",
    "    min_samples = 5  # Minimum number of samples in a neighborhood to be considered a core point\n",
    "\n",
    "    # Initialize the DBSCAN clustering model\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\n",
    "    # Fit the DBSCAN model to the data\n",
    "    dbscan.fit(df)\n",
    "\n",
    "    # Extract cluster labels (-1 represents noise points)\n",
    "    labels = dbscan.labels_\n",
    "    print(f'DBSCAN propose {len(set(labels))} clusters.')\n",
    "\n",
    "    # Convert cluster labels to integer and then to string format\n",
    "    cluster_labels_str = labels.astype(int).astype(str)\n",
    "\n",
    "    # Create a list of colors based on labels\n",
    "    if color_set is None:\n",
    "        cluster_colors = custom_color_set_4[:len(set(labels))]\n",
    "    else:\n",
    "        cluster_colors = color_set[:len(set(labels))]\n",
    "\n",
    "    # Create a 3D scatter plot with custom legend\n",
    "    fig = px.scatter_3d(df, x='recent_timestamp', y='nb_commandes', z='payment_total', color=cluster_labels_str,\n",
    "                        color_discrete_sequence=cluster_colors, opacity=opacity)\n",
    "\n",
    "    fig.update_traces(marker=dict(size=5), showlegend=legend)\n",
    "    fig.update_layout(title='DBSCAN Clustering in 3D')\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=800,  # Specify the width in pixels\n",
    "        height=600  # Specify the height in pixels\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_DBSCAN_3D()\n",
    "\n",
    "# mm ?\n",
    "\n",
    "# On passe très vide de 1 cluster à une multide de clusters,\n",
    "# pour des valeurs de eps assez proches.\n",
    "# Difficile d'obtenir juste qq clusters qui aient du sens métier.\n",
    "\n",
    "# J'ai l'impression que nos points sont espacés de manière trop régulière pour le DBSCAN,\n",
    "# la topologie n'est pas intéressante pour cet algo car\n",
    "# Il n'y a pas de zone de vide qui sépare naturellement nos données.\n",
    "\n",
    "# Ce ne sera cependant plus forcément le cas en dimensions supérieures, donc\n",
    "# DBSCAN à réessayer qd on aura ajouté des features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avec un genre de gridsearchcv\n",
    "\n",
    "# Turns out, GridSearchCV has no score really suitable for clustering evaluation\n",
    "# We need to do a \"manual gridsearch\"\n",
    "\n",
    "# Define the range of eps and min_samples values\n",
    "eps_values = [0.1, 0.2, 0.3]\n",
    "min_samples_values = [3, 5, 7]\n",
    "\n",
    "# Initialize lists to store scores and cluster counts\n",
    "dbscan_scores = []\n",
    "cluster_counts = []\n",
    "\n",
    "# Perform manual grid search\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(rfm_log_scaled)\n",
    "        # Silhouette prend bcp + de tps\n",
    "        # score = silhouette_score(rfm_log_scaled, labels)\n",
    "        # Utilisons D-B ici :\n",
    "        score = davies_bouldin_score(rfm_log_scaled[features_RFM], labels)\n",
    "        dbscan_scores.append(score)\n",
    "        nb_clusters = len(set(labels))\n",
    "        cluster_counts.append(nb_clusters)\n",
    "\n",
    "print('On trouve entre ' + str(min(cluster_counts)) + ' clusters (valeurs élevées pour eps et min_samples)')\n",
    "print('et ' + str(max(cluster_counts)) + ' clusters (valeurs faibles)')\n",
    "\n",
    "# Problème : qd le nb de clusters diminue, le score de Davies-Bouldin augmente,\n",
    "# ce qui est mauvais : cela signifie des clusters moins compacts et/ou moins séparés.\n",
    "# Il y a donc un compromis à faire entre la qualité du clustering et son utilité métier\n",
    "\n",
    "# Or on peut vérifier que pour 7 clusters le kmeans obtenait un score bien inférieur (donc meilleur)\n",
    "# Encore une fois, sur ces données, le kmeans est à la fois + rapide et plus performant\n",
    "\n",
    "# Reshape the scores and cluster counts for plotting\n",
    "dbscan_scores = np.array(dbscan_scores).reshape(len(eps_values), len(min_samples_values))\n",
    "cluster_counts = np.array(cluster_counts).reshape(len(eps_values), len(min_samples_values))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(dbscan_scores, interpolation='nearest', cmap=plt.cm.autumn_r)\n",
    "plt.title(\"DB Scores\")\n",
    "plt.xticks(np.arange(len(min_samples_values)), min_samples_values)\n",
    "plt.yticks(np.arange(len(eps_values)), eps_values)\n",
    "plt.xlabel(\"min_samples\")\n",
    "plt.ylabel(\"eps\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cluster_counts, interpolation='nearest', cmap=plt.cm.autumn_r)\n",
    "plt.title(\"Number of Clusters\")\n",
    "plt.xticks(np.arange(len(min_samples_values)), min_samples_values)\n",
    "plt.yticks(np.arange(len(eps_values)), eps_values)\n",
    "plt.xlabel(\"min_samples\")\n",
    "plt.ylabel(\"eps\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom gradual cmap,\n",
    "# higher parameter values\n",
    "\n",
    "color1 = (1.0, 1.0, 0.3)\n",
    "color2 = (1.0, 0.1, 0.1)\n",
    "\n",
    "num_segments = 256\n",
    "\n",
    "gradient_colors = [color1, color2]\n",
    "gradual_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", gradient_colors, N=num_segments)\n",
    "\n",
    "eps_values = [0.2, 0.3, 0.4]\n",
    "min_samples_values = [5, 7, 9]\n",
    "\n",
    "dbscan_scores = []\n",
    "cluster_counts = []\n",
    "\n",
    "# Perform manual grid search\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(rfm_log_scaled)\n",
    "        score = davies_bouldin_score(rfm_log_scaled[features_RFM], labels)\n",
    "        dbscan_scores.append(score)\n",
    "        nb_clusters = len(set(labels))\n",
    "        cluster_counts.append(nb_clusters)\n",
    "\n",
    "print('On trouve entre ' + str(min(cluster_counts)) + ' clusters (valeurs élevées pour eps et min_samples)')\n",
    "print('et ' + str(max(cluster_counts)) + ' clusters (valeurs faibles)')\n",
    "\n",
    "dbscan_scores = np.array(dbscan_scores).reshape(len(eps_values), len(min_samples_values))\n",
    "cluster_counts = np.array(cluster_counts).reshape(len(eps_values), len(min_samples_values))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(dbscan_scores, interpolation='nearest', cmap=gradual_cmap)\n",
    "plt.title(\"DB Scores\")\n",
    "plt.xticks(np.arange(len(min_samples_values)), min_samples_values)\n",
    "plt.yticks(np.arange(len(eps_values)), eps_values)\n",
    "plt.xlabel(\"min_samples\")\n",
    "plt.ylabel(\"eps\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cluster_counts, interpolation='nearest', cmap=gradual_cmap)\n",
    "plt.title(\"Number of Clusters\")\n",
    "plt.xticks(np.arange(len(min_samples_values)), min_samples_values)\n",
    "plt.yticks(np.arange(len(eps_values)), eps_values)\n",
    "plt.xlabel(\"min_samples\")\n",
    "plt.ylabel(\"eps\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# mm conclusion que précédemment, pour 5 clusters le score du kmeans est bien meilleur.\n",
    "# Et en plus kmeans va bcp + vite\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion features RFM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lorsqu'on prend en compte uniquement ces 3 features,\n",
    "\n",
    "# le nombre optimal de clusters que nous pouvons faire est de 4 clusters,\n",
    "# ce qui est un bon début, mais encore trop peu pour une segmentation de qualité, selon les\n",
    "# besoins d'Olis.\n",
    "# Plutôt que de \"forcer\" un sous-partitionnement sub-optimal, nous allons ajouter une quatrième feature,\n",
    "# la plus pertinente possible pour comprendre nos clients.\n",
    "\n",
    "# Nous allons donc utiliser le score (moyen) de satisfaction indiqué par les clients, qui varie entre 1 et 5.\n",
    "\n",
    "# Sur 3 dimensions, le k-means n'a pas juste été \"plus performant\" que les 2 autres algos ;\n",
    "# il les a littéralement laissés dans la poussière ! (/ mis KO ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout du review_score, export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfms = rfm_log.copy()\n",
    "rfms['satisfaction'] = rfm_complet['review_score']\n",
    "rfms['datetime'] = rfm_complet['most_recent_order']\n",
    "\n",
    "print(rfms.loc[rfms['nb_commandes'] > 0.7, :].shape[0]) # 0.693 = log(1+1)\n",
    "# OK on n'a pas perdu nos 3%\n",
    "\n",
    "quick_look(rfms)\n",
    "rfms.describe()\n",
    "\n",
    "# Il faut que je coupe ce notebook ici, ça a du sens, et c'est utile pour travailler\n",
    "# (ça commence à être trop long de recharger tt le notebook qd c nécessaire)\n",
    "\n",
    "rfms.to_csv('data/rfms.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annexes / tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian mixture models ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le t-SNE ne permet pas d'interpréter les distances ou les tailles des clusters sur le graph obtenu,\n",
    "# ce n'est donc pas du tout l'outil idéal pour une segmentation de clientèle.\n",
    "# C juste pour l'essayer !\n",
    "\n",
    "# Perform t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "tsne_result = tsne.fit_transform(rfm_log_scaled[features_RFM])\n",
    "\n",
    "# Create a DataFrame to hold the t-SNE results\n",
    "df_tsne = pd.DataFrame(tsne_result, columns=[\"Dimension 1\", \"Dimension 2\"])\n",
    "\n",
    "# Visualize the t-SNE results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df_tsne[\"Dimension 1\"], df_tsne[\"Dimension 2\"])\n",
    "plt.title(\"t-SNE Visualization\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.show()\n",
    "\n",
    "# Wow ! De toute beauté !\n",
    "\n",
    "# Après recherche, j'avais mal compris. Le t-SNE peut aider dans certains cas à détecter des clusters,\n",
    "# mais ce n'est pas un outil de clusterisation à proprement parler. Il n'assigne pas de labels.\n",
    "# C'est un outil de réduction dimentionnelle, et pour l'instant on n'en a mm pas besoin,\n",
    "# on est \"seulement\" en 3D et on n'ira pas en dimensions très élevées.\n",
    "\n",
    "# Ca reste une belle pomme, bleue comme une orange.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_p4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
